{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQt-gyAYUbm3"
      },
      "source": [
        "### AI/LLM Engineering Kick-off!! \n",
        "\n",
        "\n",
        "For our initial activity, we will be using the OpenAI Library to Programmatically Access GPT-4.1-nano!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PInACkIWUhOd"
      },
      "source": [
        "In order to get started, you'll need an OpenAI API Key. [here](https://platform.openai.com)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ecnJouXnUgKv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pOrbwSU5H_"
      },
      "source": [
        "### Our First Prompt\n",
        "\n",
        "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) if you get stuck!\n",
        "\n",
        "Let's create a `ChatCompletion` model to kick things off!\n",
        "\n",
        "There are three \"roles\" available to use:\n",
        "\n",
        "- `developer`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
        "\n",
        "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
        "\n",
        "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iy_LEPNEMVvC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofMwuUQOU4sf",
        "outputId": "7db141d5-7f7a-4f82-c9ff-6eeafe65cfa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-C4hjcla7YTABW04EqVUrav8CSosXd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks in the AI and natural language processing ecosystem, primarily designed to facilitate building applications with large language models (LLMs). However, they serve different purposes and offer distinct features. Here's a comparison to clarify their differences:\\n\\n**1. Purpose and Focus**\\n\\n- **LangChain:**\\n  - Focuses on building **end-to-end AI applications** by providing abstractions and tools for chaining together multiple language model calls, managing conversation state, prompt engineering, and integrating with various data sources.\\n  - Designed to create complex applications like chatbots, question-answering systems, and workflows that involve multiple steps and components.\\n\\n- **LlamaIndex (GPT Index):**\\n  - Primarily aims to enable **efficient retrieval-augmented generation (RAG)** by indexing large external data sources (like documents, knowledge bases) to allow LLMs to access and reason over that data.\\n  - Simplifies the process of building knowledge bases and retrieval layers for LLM-powered systems.\\n\\n**2. Core Functionality**\\n\\n- **LangChain:**\\n  - Provides components such as chains, prompts, memory modules, agents, and tools.\\n  - Supports integration with multiple LLM providers (OpenAI, Hugging Face models, Anthropic, etc.).\\n  - Facilitates complex workflows—multi-turn conversations, dynamic decision-making, and task automation.\\n  - Offers integrations with various data sources, APIs, and frameworks.\\n\\n- **LlamaIndex:**\\n  - Offers data ingestion tools to parse and index large datasets (documents, websites, databases).\\n  - Provides retrieval modules to fetch relevant information efficiently.\\n  - Simplifies embedding management, storage, and retrieval of text data.\\n  - Focuses on enabling LLMs to access structured and unstructured data to improve accuracy and relevance.\\n\\n**3. Use Cases**\\n\\n- **LangChain:**\\n  - Conversational agents\\n  - Automated workflows\\n  - Multi-step reasoning\\n  - AI assistants integrating various tools and APIs\\n\\n- **LlamaIndex:**\\n  - Building knowledge bases\\n  - Retrieval-augmented question answering\\n  - DocumentQA systems\\n  - Indexing large datasets for quick retrieval before generation\\n\\n**4. Ecosystem and Community**\\n\\n- Both have active communities, but LangChain tends to be more widespread for general application development, especially in chatbots and workflow automation.\\n- LlamaIndex is more specialized in data ingestion, indexing, and retrieval tasks tied closely to LLMs.\\n\\n---\\n\\n**Summary:**\\n\\n| Aspect                  | LangChain                                       | LlamaIndex                                   |\\n|-------------------------|-------------------------------------------------|----------------------------------------------|\\n| Primary Purpose         | Building complex AI applications and workflows| Indexing and retrieving data for LLMs     |\\n| Focus                   | Workflow orchestration, chaining models, agents| Data ingestion, indexing, retrieval        |\\n| Use Cases               | Chatbots, multi-step tasks, agent systems      | Knowledge bases, document QA, retrieval    |\\n| Integration Flexibility | High; multiple LLM providers, tools           | Data sources, large datasets, retrieval     |\\n\\n---\\n\\n**In essence:**  \\n*LangChain* is like a toolkit for orchestrating and managing complex language model applications, while *LlamaIndex* specializes in making large datasets accessible and usable by LLMs through indexing and retrieval mechanisms. They can be complementary; for example, using LlamaIndex to fetch relevant documents that LangChain then processes within a broader application.\\n\\n**Hope this helps clarify their differences!**\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1755237464, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_e91a518ddb', usage=CompletionUsage(completion_tokens=712, prompt_tokens=19, total_tokens=731, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
        "\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-7MnFhVNoT"
      },
      "source": [
        "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
        "\n",
        "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB76LJrDVgbc"
      },
      "source": [
        "##### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-vmtUV7WVOLW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"developer\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXgB_5nVky_"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
        "\n",
        "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "4yRwAWvgWFNq",
        "outputId": "777e7dcb-43e3-491a-d94a-f543e19b61e6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Great question! **LangChain** and **LlamaIndex** (formerly known as GPT Index) are both popular frameworks in the AI ecosystem designed to facilitate working with large language models (LLMs) and managing data for downstream applications. However, they serve different primary purposes and offer distinct functionalities.\n",
              "\n",
              "### **LangChain**\n",
              "\n",
              "**Overview:**\n",
              "- An open-source framework focused on building **conversational AI applications**, **chatbots**, and **complex language model-powered workflows**.\n",
              "- Provides abstractions and tools to connect LLMs with various data sources, APIs, and tools.\n",
              "- Supports the development of multi-step reasoning, chaining multiple prompts, memory management, and agent-based workflows.\n",
              "\n",
              "**Key Features:**\n",
              "- **Chains:** Modular components that pass data through multiple processing steps.\n",
              "- **Agents:** Dynamic systems that decide which tools or functions to invoke based on input.\n",
              "- **Memory:** Persist context across interactions to maintain state.\n",
              "- **Integrations:** Seamless connectivity with data sources, APIs, document stores, and more.\n",
              "- **Prompt Management:** Templates, prompt templates, and prompt engineering utilities.\n",
              "\n",
              "**Use Cases:**\n",
              "- Building chatbots and conversational agents.\n",
              "- Automating multi-step workflows involving LLMs.\n",
              "- Connecting LLMs to external tools and APIs.\n",
              "\n",
              "---\n",
              "\n",
              "### **LlamaIndex (formerly GPT Index)**\n",
              "\n",
              "**Overview:**\n",
              "- Focused primarily on **building efficient data indexes** over large collections of documents to facilitate **quick and effective retrieval** for LLMs.\n",
              "- Designed to **enhance retrieval-augmented generation (RAG)** systems by enabling LLMs to access relevant document snippets during inference.\n",
              "- Works well with various data sources and formats.\n",
              "\n",
              "**Key Features:**\n",
              "- **Data Indexing:** Converts raw data into searchable indices.\n",
              "- **Retrieval:** Fast and relevant document retrieval tailored for LLM augmentation.\n",
              "- **Memory Management:** Maintains knowledge bases for long-term context.\n",
              "- **Support for multiple index types:** Vector indices, tree indices, etc.\n",
              "- **Data ingestion pipelines:** Supports documents, PDFs, HTML, and more.\n",
              "\n",
              "**Use Cases:**\n",
              "- Building knowledge bases for chatbots or question-answering systems.\n",
              "- Retrieval-augmented generation to improve factual accuracy.\n",
              "- Managing large document corpora for LLM applications.\n",
              "\n",
              "---\n",
              "\n",
              "### **Summary of Differences**\n",
              "\n",
              "| Aspect | **LangChain** | **LlamaIndex** (GPT Index) |\n",
              "| --- | --- | --- |\n",
              "| **Primary Focus** | Building complex conversational workflows and integrating LLMs with tools/APIs | Efficient data storage, retrieval, and augmentation for LLMs |\n",
              "| **Use Cases** | Chatbots, multi-step routines, agent-based systems | Knowledge bases, RAG systems, document retrieval |\n",
              "| **Core Functionality** | Chains, agents, prompt engineering | Data indexing, document retrieval, embedding management |\n",
              "| **Data Handling** | Connects to APIs, databases, documents for workflows | Creates searchable indexes over documents for quick retrieval |\n",
              "\n",
              "---\n",
              "\n",
              "### **In essence:**\n",
              "- **LangChain** excels at orchestrating multiple steps, tools, and memory in complex language-driven applications.\n",
              "- **LlamaIndex** specializes in organizing and retrieving large amounts of data to support LLMs, especially in retrieval-augmented scenarios.\n",
              "\n",
              "They are **complementary**: one deals more with **workflow orchestration** (LangChain), and the other with **data management and retrieval** (LlamaIndex). Often, developers use both together to build powerful AI systems.\n",
              "\n",
              "---\n",
              "\n",
              "**Note:** Both frameworks are actively evolving, so it’s a good idea to check their latest documentation for updates and new features."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "messages = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPs3ScS1WpoC"
      },
      "source": [
        "Let's focus on extending this a bit, and incorporate a `developer` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The `developer` message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the `developer` prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "b744311f-e151-403e-ea8e-802697fcd4ec"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Honestly, I can't stand crushed ice! It's like biting into a handful of soggy snow while I'm already starving. Cubed ice at least pauses to give me a little chill without turning into a sloppy mess. But frankly, I’m so hungry and fed up with waiting, I’d settle for whatever gets me some cold relief—just not crushed ice!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFs56KVaXuEY"
      },
      "source": [
        "Let's try that same prompt again, but modify only our system prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "ede64a76-7006-42f1-b140-b899e389aa7d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I love the idea of crushed ice! It's so refreshing and fun to enjoy, especially in a chilly drink or a summery treat. Cubed ice is great too, especially for keeping beverages cold without diluting them too quickly. Both have their charms—depends on the mood! How about you? Do you prefer crushed or cubed ice?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
        "\n",
        "joyful_response = get_response(client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmjJd8zYQUK"
      },
      "source": [
        "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6b6z3CkYX9Y",
        "outputId": "64a425b2-d025-4079-d0a3-affd9c2d5d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-C4hwCN5dfnfA0Wm0yxafO8u7AKLuk', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I love the idea of crushed ice! It's so refreshing and fun to enjoy, especially in a chilly drink or a summery treat. Cubed ice is great too, especially for keeping beverages cold without diluting them too quickly. Both have their charms—depends on the mood! How about you? Do you prefer crushed or cubed ice?\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1755238244, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_c4c155951e', usage=CompletionUsage(completion_tokens=69, prompt_tokens=30, total_tokens=99, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMRJLbOYcwq"
      },
      "source": [
        "### Prompt Engineering\n",
        "\n",
        "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-4.1-mini` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "bab916e6-12c6-43cc-d37d-d0e01800c524"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Climate change refers to long-term alterations in Earth's climate patterns, primarily driven by human activities such as burning fossil fuels, deforestation, and industrial processes. These activities increase the concentration of greenhouse gases like carbon dioxide and methane in the atmosphere, leading to global warming. The effects include rising sea levels, more frequent and severe weather events, melting glaciers, and disruptions to ecosystems. Addressing climate change requires global cooperation to reduce emissions, transition to renewable energy sources, and promote sustainable practices to protect the planet for future generations."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Ay, grabe, mga kababayan! Ang climate change, para kang kumare na umaapak sa paa mo, hindi mo maialis, hindi mo maiwalay. Sabi nila, ang planeta natin ay nagiging mainit, parang lola mong sobra sa mainit na pan de sal, diba? Kailangan na natin mag-action! Hindi pwedeng maghintay na maging si Earth na lang ang may sinabi sa atin. Kaya, tumulong tayo—mag-recycle, mag-save ng energy, at higit sa lahat, maging mas responsible tayo bilang mga anak ng mundo. Kasi kung hindi tayo kikilos, baka sa huli, ang environment na lang ang magdusa, at tayo, mga kababayan, ang maiwan sa isang mas malungkot na planeta. Remember, ka-vibe, ang pagbabago ay nagsisimula sa ating bawat maliit na hakbang!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change as vice ganda in a talk show.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #1: Play around with the prompt using any techniques from the prompt engineering guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Few-shot Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VchCPbbedTfX"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4InUN_ArZJpa",
        "outputId": "ca294b81-a84e-4cba-fbe9-58a6d4dcc4d9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The stimple screwdriver effortlessly powered the falbean, making it easy to tighten the bolts."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zn9-X2d23Z"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUvXSWpeCs6"
      },
      "source": [
        "### Chain of Thought\n",
        "\n",
        "You'll notice that, by default, the model uses Chain of Thought to answer difficult questions!\n",
        "\n",
        "> This pattern is leveraged even more by advanced reasoning models like [`o3` and `o4-mini`](https://openai.com/index/introducing-o3-and-o4-mini/)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "cwW0IgbfeTwP",
        "outputId": "3317783b-6b23-4e38-df48-555e1a3c9fac"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "There is 1 letter \"r\" in \"strawberry?\"."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "reasoning_problem = \"\"\"\n",
        "how many r's in \"strawberry?\" {instruction}\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcrU-4pgRBS"
      },
      "source": [
        "Notice that the model cannot count properly. It counted only 2 r's."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #2: Update the prompt so that it can count correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "There are 2 letter 'r's in the word \"strawberry.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "How many letter 'r's are in the word \"strawberry\"? \n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's go through each letter of the word \"strawberry\" step by step and count the number of times the letter 'r' appears:\n",
              "\n",
              "1. **s** – not 'r'  \n",
              "2. **t** – not 'r'  \n",
              "3. **r** – **1st 'r'**  \n",
              "4. **a** – not 'r'  \n",
              "5. **w** – not 'r'  \n",
              "6. **b** – not 'r'  \n",
              "7. **e** – not 'r'  \n",
              "8. **r** – **2nd 'r'**  \n",
              "9. **r** – **3rd 'r'**  \n",
              "10. **y** – not 'r'  \n",
              "\n",
              "**Total number of 'r's in \"strawberry\": 3**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "Please go through each letter of the word strawberry step by step and count how many times the letter 'r' appears.\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's go through each letter of the word \"strawberry\" step by step:\n",
              "\n",
              "1. 's' – not 'r'\n",
              "2. 't' – not 'r'\n",
              "3. 'r' – yes, count = 1\n",
              "4. 'a' – not 'r'\n",
              "5. 'w' – not 'r'\n",
              "6. 'b' – not 'r'\n",
              "7. 'e' – not 'r'\n",
              "8. 'r' – yes, count = 2\n",
              "9. 'r' – yes, count = 3'\n",
              "10. 'y' – not 'r'\n",
              "\n",
              "Total number of 'r's in \"strawberry\" is **3**."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "How many letter 'r's are in the word \"strawberry\"? \n",
        "Please go through each letter of the word step by step and count how many times the letter 'r' appears.\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Sure! Let's examine each letter of the word \"strawberry\" step by step and count how many times the letter \"r\" appears.\n",
              "\n",
              "The word is: **s t r a w b e r r y**\n",
              "\n",
              "Now, proceeding letter by letter:\n",
              "\n",
              "1. **s** — Not an \"r\"; count remains 0.\n",
              "2. **t** — Not an \"r\"; count remains 0.\n",
              "3. **r** — This is an \"r\"; count increases to 1.\n",
              "4. **a** — Not an \"r\"; count remains 1.\n",
              "5. **w** — Not an \"r\"; count remains 1.\n",
              "6. **b** — Not an \"r\"; count remains 1.\n",
              "7. **e** — Not an \"r\"; count remains 1.\n",
              "8. **r** — This is an \"r\"; count increases to 2.\n",
              "9. **r** — This is an \"r\"; count increases to 3.\n",
              "10. **y** — Not an \"r\"; count remains 3.\n",
              "\n",
              "**Final count:** The letter **\"r\"** appears **3 times** in the word \"strawberry.\"\n",
              "\n",
              "**Reasoning Recap:** I checked each letter individually, increased the count whenever I encountered an \"r,\" and kept track throughout the process."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "reasoning_problem = \"\"\"\n",
        "Please go through each letter of the word \"strawberry\" step by step and focus on counting how many times the letter r appears. Explain your reasoning for your number count {instruction}\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basic Prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Prompt engineering is the process of designing, refining, and optimizing input prompts to effectively communicate with AI language models—such as GPT—to generate accurate, relevant, and high-quality responses. It involves carefully crafting the wording, structure, and context of prompts to guide the model's output in a desired direction, often by providing specific instructions, examples, or constraints.\n",
              "\n",
              "Key aspects of prompt engineering include:\n",
              "- **Clarity:** Making prompts clear and unambiguous to elicit precise responses.\n",
              "- **Context:** Providing sufficient background information to inform the model's understanding.\n",
              "- **Instruction Framing:** Using explicit commands or questions to guide the model’s behavior.\n",
              "- **Iterative Refinement:** Testing and adjusting prompts based on the outputs to improve results.\n",
              "- **Use of Techniques:** Such as few-shot learning (providing examples), zero-shot prompts, and chain-of-thought prompting to enhance performance.\n",
              "\n",
              "Prompt engineering is vital in applications like chatbots, content generation, coding assistance, and more—helping users leverage AI models more effectively and reliably."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"What is Prompt Engineering.\")\n",
        "]\n",
        "\n",
        "response = get_response(client, list_of_prompts)\n",
        "pretty_print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chain of Thought\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Great! To start, could you tell me what, if anything, you already know about prompt engineering? Have you used AI models before, or are you completely new to the concept? \n",
              "\n",
              "Once I understand your background, I can better tailor my explanations. For example, are you interested in creating prompts for specific tasks like writing, coding, or data analysis? Or are you more curious about how to craft prompts that guide the AI effectively and ethically? \n",
              "\n",
              "In the meantime, I can introduce a few key techniques used in prompt engineering:\n",
              "\n",
              "1. **Few-shot prompting:** This involves providing a few examples within your prompt to help the model understand the pattern or task. For example, if you want the AI to translate phrases, you might show a few sample translations before asking for a new one.\n",
              "\n",
              "2. **Clarifying questions:** Sometimes, it helps to add questions or specify constraints to narrow down the AI's responses, ensuring they are relevant and precise.\n",
              "\n",
              "3. **Bias reduction:** Being careful with wording and examples to avoid inadvertently reinforcing stereotypes or biases in the AI’s responses.\n",
              "\n",
              "Would you like to explore any of these techniques in more detail? Or do you have specific goals for your prompt engineering practice?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "reasoning_problem = \"\"\"\n",
        "“You are an expert tutor explaining prompt engineering to a beginner. Ask questions to understand what the learner knows. \n",
        "Provide a few examples to illustrate different techniques (like few-shot prompting, clarifying questions, or bias reduction). \n",
        "Ensure your answer is unbiased and avoids oversimplification.”\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k9TKR1DhWI2"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Now that you're accessing `gpt-4.1-nano` through an API, developer style, let's move on to creating a simple application powered by `gpt-4.1-nano`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Materials adapted for PSI AI Academy. Original materials from AI Makerspace."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
