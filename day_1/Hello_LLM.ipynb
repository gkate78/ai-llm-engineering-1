{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQt-gyAYUbm3"
      },
      "source": [
        "### AI/LLM Engineering Kick-off!! \n",
        "\n",
        "\n",
        "For our initial activity, we will be using the OpenAI Library to Programmatically Access GPT-4.1-nano!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PInACkIWUhOd"
      },
      "source": [
        "In order to get started, you'll need an OpenAI API Key. [here](https://platform.openai.com)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ecnJouXnUgKv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pOrbwSU5H_"
      },
      "source": [
        "### Our First Prompt\n",
        "\n",
        "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) if you get stuck!\n",
        "\n",
        "Let's create a `ChatCompletion` model to kick things off!\n",
        "\n",
        "There are three \"roles\" available to use:\n",
        "\n",
        "- `developer`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
        "\n",
        "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
        "\n",
        "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iy_LEPNEMVvC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofMwuUQOU4sf",
        "outputId": "7db141d5-7f7a-4f82-c9ff-6eeafe65cfa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-BzajflfFF8VhHWpvYFtgB7QqljjmT', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"LangChain and LlamaIndex (formerly known as GPT Index) are both frameworks designed to facilitate working with language models and building applications that leverage large language models (LLMs). However, they serve different primary purposes and have distinct features. Here's an overview of their differences:\\n\\n**1. Purpose and Focus:**\\n\\n- **LangChain:**\\n  - Focuses on building complex, multi-step language model applications, such as chatbots, agents, and workflows.\\n  - Provides tools for chaining multiple prompts, managing conversations, handling memory, and integrating with various data sources.\\n  - Emphasizes the orchestration and control flow around LLMs, enabling developers to create sophisticated AI applications.\\n\\n- **LlamaIndex (GPT Index):**\\n  - Primarily designed for creating and querying indices over large document datasets.\\n  - Facilitates assembling large corpora into efficient data structures that can be queried with LLMs.\\n  - Focuses on data ingestion, indexing, and retrieval to enable context-aware question answering and knowledge extraction from custom datasets.\\n\\n**2. Core Functionality:**\\n\\n- **LangChain:**\\n  - Provides a suite of modules for:\\n    - Prompt management and templating\\n    - Memory and state management for conversations\\n    - Chains and agents for multi-step reasoning\\n    - Integrations with various LLM providers and external tools\\n  - Supports building conversational agents, chatbots, and autonomous agents that can perform tasks.\\n\\n- **LlamaIndex:**\\n  - Offers tools for:\\n    - Ingesting documents from various formats\\n    - Building optimized indices (e.g., GPTTreeIndex, GPTSimpleIndex)\\n    - Performing fast retrieval and querying over large datasets\\n    - Integrating with multiple data sources for knowledge management\\n\\n**3. Use Cases:**\\n\\n- **LangChain:**\\n  - Chatbots and conversational AI\\n  - Workflow automation involving LLMs\\n  - Multi-turn dialogue systems\\n  - Integrating external tools and APIs within language applications\\n\\n- **LlamaIndex:**\\n  - Creating knowledge bases from documents\\n  - Building question-answering systems over proprietary data\\n  - Summarization and information retrieval tasks in large document collections\\n\\n**4. Integration and Ecosystem:**\\n\\n- **LangChain:**\\n  - Extensible with numerous integrations and supports multiple LLM providers\\n  - Well-suited for complex application logic, agent systems, and dialogue management\\n\\n- **LlamaIndex:**\\n  - Focused on data ingestion, indexing, and retrieval; can be integrated with LLMs for enhanced QA systems\\n  - Optimized for managing large document datasets and enabling efficient querying\\n\\n---\\n\\n**Summary Table:**\\n\\n| Aspect                 | LangChain                                         | LlamaIndex (GPT Index)                                |\\n|------------------------|--------------------------------------------------|------------------------------------------------------|\\n| Primary focus          | Application orchestration and workflows         | Document indexing and retrieval                      |\\n| Use cases              | Chatbots, agents, multi-step workflows           | Knowledge bases, document QA, info retrieval       |\\n| Core features          | Chains, memory, prompt management, tool integration | Document ingestion, index construction, querying  |\\n| Typical users          | Developers building multi-step AI apps          | Data scientists, knowledge managers, Q&A systems  |\\n\\n---\\n\\n**In essence:**  \\n- Use **LangChain** when you need to build complex interactive applications, workflows, or agents that involve conversations, external tools, and multi-step processes.  \\n- Use **LlamaIndex** when your goal is to ingest large volumes of documents and enable efficient retrieval and question answering over that data.\\n\\n---\\n\\nIf you have a specific use case, I can help suggest which framework might be better suited!\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1754018919, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=744, prompt_tokens=19, total_tokens=763, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
        "\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-7MnFhVNoT"
      },
      "source": [
        "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
        "\n",
        "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB76LJrDVgbc"
      },
      "source": [
        "##### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-vmtUV7WVOLW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"developer\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXgB_5nVky_"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
        "\n",
        "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "4yRwAWvgWFNq",
        "outputId": "777e7dcb-43e3-491a-d94a-f543e19b61e6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Great question! LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks in the ecosystem of building applications with large language models (LLMs), but they serve different primary purposes and have distinct features.\n",
              "\n",
              "### **LangChain**\n",
              "- **Purpose:** Framework for building LLM-powered applications, especially focusing on **prompt management, chaining multiple operations**, and integrating with various tools and data sources.\n",
              "- **Main Features:**\n",
              "  - **Prompt management:** Helps construct, manage, and optimize prompts.\n",
              "  - **Chains:** Enables chaining of multiple LLM calls, including conditional logic and workflows.\n",
              "  - **Agents:** Supports dynamic decision-making, where the system chooses which tools or actions to invoke based on input.\n",
              "  - **Tool Integration:** Easily integrates with APIs, databases, and other tools.\n",
              "  - **Support for various LLM providers:** OpenAI, Hugging Face, local models, etc.\n",
              "  - **Extensibility:** Modular design allowing custom components.\n",
              "- **Use Cases:** Conversational agents, complex multi-step workflows, multi-tool orchestration.\n",
              "\n",
              "### **LlamaIndex (GPT Index)**\n",
              "- **Purpose:** Focused on **building and querying large, structured knowledge bases** from proprietary or external data sources like documents, PDFs, or other unstructured data, making it easier to perform retrieval-augmented generation (RAG).\n",
              "- **Main Features:**\n",
              "  - **Data ingestion:** Indexes raw data from various sources.\n",
              "  - **Index types:** Supports multiple index types (e.g., tree, list, vector) for different retrieval strategies.\n",
              "  - **Retrieval + LLM:** Combines document retrieval with LLMs to generate knowledgeable responses.\n",
              "  - **Easy querying:** Simple API to query the index for answers based on underlying data.\n",
              "  - **Knowledge augmentation:** Enables creation of chatbots that answer based on your custom knowledge base rather than general information.\n",
              "- **Use Cases:** Building custom knowledge bases, document question answering, retrieval-augmented generation.\n",
              "\n",
              "---\n",
              "\n",
              "### **Summary of Differences**\n",
              "| Aspect | LangChain | LlamaIndex (GPT Index) |\n",
              "|---|---|---|\n",
              "| **Main Focus** | Building complex LLM applications with workflows and tool integration | Creating and querying knowledge bases from external data sources |\n",
              "| **Core Functionality** | Chaining, prompting, tool orchestration, agent-based reasoning | Data ingestion, indexing, retrieval-augmented generation |\n",
              "| **Primary Use Cases** | Conversational agents, multi-step workflows, tool orchestration | Document QA, knowledge base creation, document retrieval + generation |\n",
              "| **Integration Approach** | Modular, supports multiple LLM providers, complex workflows | Focused on data ingestion, indexing, fast retrieval |\n",
              "- **Overlap:** Both can be combined—for instance, using LlamaIndex for data retrieval within a LangChain-powered application.\n",
              "\n",
              "### **In essence:**\n",
              "- Use **LangChain** if you're building sophisticated LLM applications that require prompt management, chaining, and tool integration.\n",
              "- Use **LlamaIndex** if you need to build a system that handles large collections of documents or data, allowing LLMs to retrieve and respond based on that specific knowledge.\n",
              "\n",
              "---\n",
              "\n",
              "**Note:** Both tools are evolving rapidly, and the ecosystem often sees integrations where they complement each other."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "messages = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPs3ScS1WpoC"
      },
      "source": [
        "Let's focus on extending this a bit, and incorporate a `developer` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The `developer` message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the `developer` prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "b744311f-e151-403e-ea8e-802697fcd4ec"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Are you kidding me? Neither! I want my ice in nice, big, solid chunks that stay cold and don't drown my drink in water. If I have to choose, I guess crushed ice is the worst because it melts so fast and makes everything watery. But honestly, I’d rather have none at all if it’s just going to be a mess!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFs56KVaXuEY"
      },
      "source": [
        "Let's try that same prompt again, but modify only our system prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "ede64a76-7006-42f1-b140-b899e389aa7d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I think crushed ice is fun and refreshing, especially for drinks like cocktails or slushies! Cubed ice is great for keeping beverages cold without quickly watering them down. Both have their own charm—what's your favorite?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
        "\n",
        "joyful_response = get_response(client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmjJd8zYQUK"
      },
      "source": [
        "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6b6z3CkYX9Y",
        "outputId": "64a425b2-d025-4079-d0a3-affd9c2d5d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-BzakuI20VBKRYWYXI4WuDx7XbEaPt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I think crushed ice is fun and refreshing, especially for drinks like cocktails or slushies! Cubed ice is great for keeping beverages cold without quickly watering them down. Both have their own charm—what's your favorite?\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1754018996, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=45, prompt_tokens=30, total_tokens=75, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMRJLbOYcwq"
      },
      "source": [
        "### Prompt Engineering\n",
        "\n",
        "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-4.1-mini` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "bab916e6-12c6-43cc-d37d-d0e01800c524"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Climate change refers to long-term shifts in weather patterns and global temperatures, primarily caused by human activities such as burning fossil fuels, deforestation, and industrial processes. These activities increase greenhouse gas concentrations in the atmosphere, leading to global warming. The impacts of climate change include more frequent and severe weather events, rising sea levels, melting glaciers, and disruptions to ecosystems and agriculture. Addressing climate change requires global cooperation to reduce emissions, transitioning to renewable energy sources, and implementing sustainable practices to protect the planet for future generations."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Oo, mga beks! Kagaya ng sinasabi ni Vice Ganda, eh, mukhang kailangan na nating magising sa katotohanan—ang climate change ay tunay na seryosong usapin! Kasi, mga friends, hindi natin pwedeng ipasa hanggang sa susunod na generation ang problemang ito kung hindi natin aaksyunan ngayon. Let's do our part, mag-recycle, mag-save ng energy, at higit sa lahat, magkaisa tayo para sa isang mas malinis at mas maaliwalas na planeta. Kasi, di ba, ang beauty ng buhay ay nandiyan lang sa paligid natin—kailangan lang nating ingatan para tuloy-tuloy ang saya! Charot!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change as vice ganda in a talk show.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #1: Play around with the prompt using any techniques from the prompt engineering guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Few-shot Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VchCPbbedTfX"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4InUN_ArZJpa",
        "outputId": "ca294b81-a84e-4cba-fbe9-58a6d4dcc4d9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The stimple wrench effortlessly spun the falbean, allowing us to tighten the bolt with ease."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zn9-X2d23Z"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUvXSWpeCs6"
      },
      "source": [
        "### Chain of Thought\n",
        "\n",
        "You'll notice that, by default, the model uses Chain of Thought to answer difficult questions!\n",
        "\n",
        "> This pattern is leveraged even more by advanced reasoning models like [`o3` and `o4-mini`](https://openai.com/index/introducing-o3-and-o4-mini/)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "cwW0IgbfeTwP",
        "outputId": "3317783b-6b23-4e38-df48-555e1a3c9fac"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "There are 2 letter \"r\"s in \"strawberry.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "reasoning_problem = \"\"\"\n",
        "how many r's in \"strawberry?\" {instruction}\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcrU-4pgRBS"
      },
      "source": [
        "Notice that the model cannot count properly. It counted only 2 r's."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #2: Update the prompt so that it can count correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "There are two letter 'r's in the word \"strawberry.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "How many letter 'r's are in the word \"strawberry\"? \n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's examine each letter of the word \"strawberry\" step by step and count the number of times the letter 'r' appears:\n",
              "\n",
              "1. **S** – not 'r'\n",
              "2. **t** – not 'r'\n",
              "3. **r** – 1st 'r'\n",
              "4. **a** – not 'r'\n",
              "5. **w** – not 'r'\n",
              "6. **b** – not 'r'\n",
              "7. **e** – not 'r'\n",
              "8. **r** – 2nd 'r'\n",
              "9. **r** – 3rd 'r'\n",
              "10. **y** – not 'r'\n",
              "\n",
              "**Total number of times 'r' appears: 3**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "Please go through each letter of the word strawberry step by step and count how many times the letter 'r' appears.\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's go through the word \"strawberry\" letter by letter:\n",
              "\n",
              "1. s – not 'r'\n",
              "2. t – not 'r'\n",
              "3. r – **1st 'r'**\n",
              "4. a – not 'r'\n",
              "5. w – not 'r'\n",
              "6. b – not 'r'\n",
              "7. e – not 'r'\n",
              "8. r – **2nd 'r'**\n",
              "9. r – **3rd 'r'**\n",
              "10. y – not 'r'\n",
              "\n",
              "Counting all the 'r's, we find that there are **3** in the word \"strawberry\"."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "How many letter 'r's are in the word \"strawberry\"? \n",
        "Please go through each letter of the word step by step and count how many times the letter 'r' appears.\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's analyze the word \"strawberry\" letter by letter and count how many times the letter \"r\" appears:\n",
              "\n",
              "1. **s** - Not an \"r,\" so count remains 0.\n",
              "2. **t** - Not an \"r,\" count remains 0.\n",
              "3. **r** - This is an \"r,\" so count increases to 1.\n",
              "4. **a** - Not an \"r,\" count remains 1.\n",
              "5. **w** - Not an \"r,\" count remains 1.\n",
              "6. **b** - Not an \"r,\" count remains 1.\n",
              "7. **e** - Not an \"r,\" count remains 1.\n",
              "8. **r** - This is an \"r,\" so count increases to 2.\n",
              "9. **r** - Again, an \"r,\" so count increases to 3.\n",
              "10. **y** - Not an \"r,\" count remains 3.\n",
              "\n",
              "**Final count:** The letter \"r\" appears **3 times** in \"strawberry.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "reasoning_problem = \"\"\"\n",
        "Please go through each letter of the word \"strawberry\" step by step and focus on counting how many times the letter r appears. Explain your reasoning for your number count {instruction}\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basic Prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Prompt engineering is the process of designing and refining input instructions or prompts to effectively communicate with artificial intelligence (AI) models, particularly large language models (LLMs), to produce desired outputs. It involves crafting clear, specific, and well-structured prompts to guide the AI's responses in a way that aligns with the user's goals.\n",
              "\n",
              "Key aspects of prompt engineering include:\n",
              "\n",
              "- **Clarity:** Ensuring prompts are unambiguous and straightforward.\n",
              "- **Specificity:** Providing detailed context or instructions to elicit precise answers.\n",
              "- **Formatting:** Using techniques like bullet points, questions, or examples to improve understanding.\n",
              "- **Iteration:** Refining prompts through trial and error to optimize results.\n",
              "\n",
              "Prompt engineering is essential for leveraging AI effectively in applications like content creation, coding assistance, data analysis, and more. It helps users obtain relevant, accurate, and useful outputs from AI systems."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"What is Prompt Engineering.\")\n",
        "]\n",
        "\n",
        "response = get_response(client, list_of_prompts)\n",
        "pretty_print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chain of Thought\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Great! To start, I'd like to understand your current level of familiarity. Have you heard of prompt engineering before, or is this your first time encountering the term? Also, do you have any experience working with AI models like chatbots or language models?\n",
              "\n",
              "Once I have a sense of your background, I can tailor my explanation. For now, let me ask:\n",
              "\n",
              "- Are you interested in how prompts influence the responses of models like GPT?\n",
              "- Do you want to learn about specific techniques to improve prompt quality?\n",
              "- Are you curious about how to guide the model to produce more accurate or unbiased answers?\n",
              "\n",
              "To give you a sense of what prompt engineering involves, here are some techniques illustrated with examples:\n",
              "\n",
              "**1. Few-shot Prompting:**  \n",
              "This method provides a few examples within the prompt to guide the model toward the desired task.\n",
              "\n",
              "*Example:*  \n",
              "Suppose you want the model to translate English to French. You might write:  \n",
              "```\n",
              "English: Hello  \n",
              "French: Bonjour\n",
              "\n",
              "English: How are you?  \n",
              "French: Comment ça va?\n",
              "\n",
              "English: Thank you  \n",
              "French:\n",
              "```  \n",
              "The model is likely to complete with \"Merci,\" based on the pattern.\n",
              "\n",
              "**2. Clarifying Questions:**  \n",
              "Including questions in your prompt can help the model understand context or specify the kind of answer you're seeking.\n",
              "\n",
              "*Example:*  \n",
              "Instead of asking, \"Tell me about climate change,\" you can ask:  \n",
              "\"What specific aspects of climate change are you interested in? Environmental impacts, policy responses, or technological solutions?\"\n",
              "\n",
              "**3. Bias Reduction:**  \n",
              "To reduce bias or ensure more neutral responses, you can frame your prompt carefully or ask the model to consider multiple viewpoints.\n",
              "\n",
              "*Example:*  \n",
              "\"Present arguments for and against the use of nuclear energy, without taking a side.\"\n",
              "\n",
              "Would you like to go through a specific example or technique in more detail? Or perhaps you want suggestions tailored to a particular task you're working on?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "reasoning_problem = \"\"\"\n",
        "“You are an expert tutor explaining prompt engineering to a beginner. Ask questions to understand what the learner knows. \n",
        "Provide a few examples to illustrate different techniques (like few-shot prompting, clarifying questions, or bias reduction). \n",
        "Ensure your answer is unbiased and avoids oversimplification.”\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k9TKR1DhWI2"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Now that you're accessing `gpt-4.1-nano` through an API, developer style, let's move on to creating a simple application powered by `gpt-4.1-nano`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Materials adapted for PSI AI Academy. Original materials from AI Makerspace."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
