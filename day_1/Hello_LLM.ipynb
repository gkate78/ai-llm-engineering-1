{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQt-gyAYUbm3"
      },
      "source": [
        "### AI/LLM Engineering Kick-off!! \n",
        "\n",
        "\n",
        "For our initial activity, we will be using the OpenAI Library to Programmatically Access GPT-4.1-nano!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PInACkIWUhOd"
      },
      "source": [
        "In order to get started, you'll need an OpenAI API Key. [here](https://platform.openai.com)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ecnJouXnUgKv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pOrbwSU5H_"
      },
      "source": [
        "### Our First Prompt\n",
        "\n",
        "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) if you get stuck!\n",
        "\n",
        "Let's create a `ChatCompletion` model to kick things off!\n",
        "\n",
        "There are three \"roles\" available to use:\n",
        "\n",
        "- `developer`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
        "\n",
        "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
        "\n",
        "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iy_LEPNEMVvC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofMwuUQOU4sf",
        "outputId": "7db141d5-7f7a-4f82-c9ff-6eeafe65cfa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-BxRug6GDxLPTiaGTOzPLkVJgzE3eZ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks designed to facilitate the development of large language model (LLM) applications, especially those involving data integration and retrieval. However, they serve different purposes and have distinct features:\\n\\n**1. Purpose and Focus:**\\n\\n- **LangChain:**\\n  - **Primary Focus:** Building customizable, multi-step LLM applications such as chatbots, question-answering systems, and agents.\\n  - **Core Strength:** Orchestrating LLM calls, managing prompts, conversation memory, chains, agents, and integrations with various data sources.\\n  - **Use Cases:** Complex workflows, reasoning chains, retrieval-augmented generation (RAG), agents that can decide actions, and automation.\\n\\n- **LlamaIndex (GPT Index):**\\n  - **Primary Focus:** Facilitating efficient indexing, retrieval, and querying of large personal or enterprise data repositories using LLMs.\\n  - **Core Strength:** Creating vector or keyword-based indices from documents, enabling fast retrieval of relevant information to feed into LLMs.\\n  - **Use Cases:** Building knowledge bases, document search, and retrieval-augmented tools that extract relevant data from large document collections.\\n\\n**2. Architecture and Components:**\\n\\n- **LangChain:**\\n  - Modular components: chains, prompts, memory, agents, tools.\\n  - Supports various LLM providers and APIs.\\n  - Powerful for constructing sequential or reasoning workflows.\\n\\n- **LlamaIndex:**\\n  - Index structures: keyword, vector, or hybrid indices.\\n  - Data ingestion pipelines for various document types.\\n  - Query interfaces tailored for fast retrieval and augmentation.\\n\\n**3. Integration and Extensibility:**\\n\\n- **LangChain:**\\n  - Highly extensible with support for custom chains, prompts, tools, and integrations.\\n  - Focuses on flexibility in designing complex workflows.\\n\\n- **LlamaIndex:**\\n  - Emphasizes linking document data with LLMs for retrieval tasks.\\n  - Supports integration with document storage systems, embeddings, and vector databases.\\n\\n**4. Typical Usage Scenarios:**\\n\\n| Aspect                         | LangChain                                              | LlamaIndex                                              |\\n|--------------------------------|--------------------------------------------------------|----------------------------------------------------------|\\n| Application type               | Chatbots, reasoning agents, complex pipelines       | Document retrieval, knowledge bases, data augmentation |\\n| Data handling                  | Orchestrates data retrieval within workflows        | Builds indices from data sources for fast lookup      |\\n| Custom workflows               | Yes                                                    | Limited to data indexing and retrieval tasks            |\\n\\n---\\n\\n**In summary:**\\n\\n- **LangChain** is a versatile framework for building complex LLM-driven applications with customizable workflows, chaining, and agent capabilities.\\n- **LlamaIndex** specializes in creating efficient indices over large datasets to facilitate quick retrieval and augmentation of information using LLMs.\\n\\nThey can be used together: LlamaIndex can handle data indexing and retrieval, while LangChain can orchestrate the overall application logic incorporating these retrievals as part of larger workflows.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1753508350, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=609, prompt_tokens=19, total_tokens=628, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
        "\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-7MnFhVNoT"
      },
      "source": [
        "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
        "\n",
        "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB76LJrDVgbc"
      },
      "source": [
        "##### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-vmtUV7WVOLW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"developer\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXgB_5nVky_"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
        "\n",
        "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "4yRwAWvgWFNq",
        "outputId": "777e7dcb-43e3-491a-d94a-f543e19b61e6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks designed to facilitate building applications that leverage large language models (LLMs), but they have different focuses and design philosophies. Here's a breakdown of their main differences:\n",
              "\n",
              "1. Purpose and Focus:\n",
              "   - **LangChain:**  \n",
              "     Primarily a framework for building **chatbots and conversational AI applications**. It provides tools for managing prompts, chaining multiple LLM calls, memory management, and integrating external tools or APIs. Its goal is to enable developers to build complex, multi-step workflows with LLMs.\n",
              "   \n",
              "   - **LlamaIndex:**  \n",
              "     Focused on **indexing and querying large collections of external data** (e.g., documents, databases) using LLMs. Its main aim is to facilitate **semantic search and question-answering** over custom data sources by creating structured indices that can efficiently retrieve relevant information for LLMs to generate responses.\n",
              "\n",
              "2. Core Functionality:\n",
              "   - **LangChain:**  \n",
              "     - Chains and agents that combine multiple prompts and model calls  \n",
              "     - Memory management to maintain context across interactions  \n",
              "     - Integration with APIs, tools, and external data sources  \n",
              "     - Flexible prompt engineering and orchestration of workflows\n",
              "   \n",
              "   - **LlamaIndex:**  \n",
              "     - Data ingestion pipelines for documents, PDFs, and databases  \n",
              "     - Indexing structures (such as trees, vectors) that enable fast retrieval  \n",
              "     - Query interfaces that leverage these indices to answer questions contextually  \n",
              "     - Emphasis on building a \"knowledge base\" to enhance LLM performance on domain-specific data\n",
              "\n",
              "3. Use Cases:\n",
              "   - **LangChain:**  \n",
              "     - Building chatbots, virtual assistants, or complex decision-making workflows  \n",
              "     - Automating multi-step tasks involving LLMs and external tools  \n",
              "     - Developing applications that require dynamic prompt construction and execution\n",
              "   \n",
              "   - **LlamaIndex:**  \n",
              "     - Creating search engines over personal or enterprise data  \n",
              "     - Building knowledge bases and document retrieval systems  \n",
              "     - Enhancing question-answering systems with structured data retrieval\n",
              "\n",
              "4. Ecosystem and Integration:\n",
              "   - **LangChain:**  \n",
              "     - Supports a wide variety of LLM providers (OpenAI, Hugging Face models, etc.)  \n",
              "     - Extensive integrations with APIs, tools, and databases  \n",
              "   \n",
              "   - **LlamaIndex:**  \n",
              "     - Primarily focused on data ingestion and retrieval  \n",
              "     - Can work with various data storage formats and retrieval mechanisms\n",
              "\n",
              "**In summary:**  \n",
              "- Use **LangChain** if you're building complex conversational systems, workflows, or need orchestration across multiple tools.  \n",
              "- Use **LlamaIndex** if your goal is to index large data collections and perform efficient semantic searches or question-answering over those datasets.\n",
              "\n",
              "They can also complement each other in a full-stack application: LlamaIndex for indexing and retrieval of domain data, and LangChain for orchestrating conversations and integrating external tools."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "messages = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPs3ScS1WpoC"
      },
      "source": [
        "Let's focus on extending this a bit, and incorporate a `developer` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The `developer` message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the `developer` prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "b744311f-e151-403e-ea8e-802697fcd4ec"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Are you kidding me? I don't have time to waste on such trivial nonsense while I'm starving! Crushed ice all the way—it's instantly satisfying and refreshingly chaotic! Can't stand those boring, clunky cubes when I'm this hungry!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFs56KVaXuEY"
      },
      "source": [
        "Let's try that same prompt again, but modify only our system prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "ede64a76-7006-42f1-b140-b899e389aa7d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I love the idea of crushed ice—it's so refreshing and fun to crunch! But cubed ice is great too, especially for neat presentations or slower melting drinks. It really depends on the mood, but either way, ice makes everything cooler!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
        "\n",
        "joyful_response = get_response(client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmjJd8zYQUK"
      },
      "source": [
        "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6b6z3CkYX9Y",
        "outputId": "64a425b2-d025-4079-d0a3-affd9c2d5d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-Bm0TwsXHyCObslTRMC8qayceKYajt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I think crushed ice is fun because it feels like a cool burst of refreshment all at once! Cubed ice is great for keeping drinks cold without diluting them too quickly and looks nice in a glass. Both have their charms—what's your favorite?\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1750781296, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=52, prompt_tokens=30, total_tokens=82, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMRJLbOYcwq"
      },
      "source": [
        "### Prompt Engineering\n",
        "\n",
        "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-4.1-mini` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "bab916e6-12c6-43cc-d37d-d0e01800c524"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Climate change refers to long-term shifts in temperature, precipitation, and other atmospheric patterns primarily caused by human activities such as burning fossil fuels, deforestation, and industrial processes. These activities elevate greenhouse gas levels in the atmosphere, leading to global warming. The impacts of climate change include more frequent and severe weather events, rising sea levels, melting glaciers, and disruptions to ecosystems and agriculture. Addressing climate change requires coordinated efforts to reduce emissions, transition to renewable energy sources, and implement sustainable practices to protect the planet for future generations."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Ay naku, mga kaibigan! Nakakabahala talaga itong climate change, parang kilabot na bagyo na hindi titigil! Sobrang init na, para kang laging nasa kiliti ng araw, tapos biglang uulan ng mga sakuna. Ang mundo natin, nagiging paasa sa ating mga kamay, parang crush na laging manhid. Kailangan na nating kumilos, mag-recycle, mag-walk if pwede, at huwag kalimutang mag-alaga sa kalikasan. Dahil kung hindi tayo kikilos, baka kayo na lang ang may kasalanan kung mas lalo pang lumala ang problema. Tandaan, mga kaibigan, sama-samang paglaban sa climate change, susi sa malinis na kinabukasan!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change as vice ganda in a talk show.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #1: Play around with the prompt using any techniques from the prompt engineering guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Few-shot Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VchCPbbedTfX"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4InUN_ArZJpa",
        "outputId": "ca294b81-a84e-4cba-fbe9-58a6d4dcc4d9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The stimple screwdriver smoothly engaged with the falbean to securely tighten the bolt."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zn9-X2d23Z"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUvXSWpeCs6"
      },
      "source": [
        "### Chain of Thought\n",
        "\n",
        "You'll notice that, by default, the model uses Chain of Thought to answer difficult questions!\n",
        "\n",
        "> This pattern is leveraged even more by advanced reasoning models like [`o3` and `o4-mini`](https://openai.com/index/introducing-o3-and-o4-mini/)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "cwW0IgbfeTwP",
        "outputId": "3317783b-6b23-4e38-df48-555e1a3c9fac"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "There are 2 letters 'r' in \"strawberry\"."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "reasoning_problem = \"\"\"\n",
        "how many r's in \"strawberry?\" {instruction}\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcrU-4pgRBS"
      },
      "source": [
        "Notice that the model cannot count properly. It counted only 2 r's."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #2: Update the prompt so that it can count correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's analyze the word \"strawberry\" letter by letter and count how many times the letter \"r\" appears:\n",
              "\n",
              "1. **s** - Not an \"r,\" so count remains 0.\n",
              "2. **t** - Not an \"r,\" count remains 0.\n",
              "3. **r** - This is an \"r,\" so count increases to 1.\n",
              "4. **a** - Not an \"r,\" count remains 1.\n",
              "5. **w** - Not an \"r,\" count remains 1.\n",
              "6. **b** - Not an \"r,\" count remains 1.\n",
              "7. **e** - Not an \"r,\" count remains 1.\n",
              "8. **r** - This is an \"r,\" so count increases to 2.\n",
              "9. **r** - Again, an \"r,\" so count increases to 3.\n",
              "10. **y** - Not an \"r,\" count remains 3.\n",
              "\n",
              "**Final count:** The letter \"r\" appears **3 times** in \"strawberry.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "reasoning_problem = \"\"\"\n",
        "Please go through each letter of the word \"strawberry\" step by step and focus on counting how many times the letter r appears. Explain your reasoning for your number count {instruction}\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k9TKR1DhWI2"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Now that you're accessing `gpt-4.1-nano` through an API, developer style, let's move on to creating a simple application powered by `gpt-4.1-nano`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Materials adapted for PSI AI Academy. Original materials from AI Makerspace."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
