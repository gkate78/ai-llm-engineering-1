{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "88b48542",
      "metadata": {
        "id": "88b48542"
      },
      "source": [
        "## Chapter 0: TL;DR\n",
        "\n",
        "### Spoilers\n",
        "\n",
        "In this short chapter, we'll get right to it and fine-tune a small-ish language model, Microsoft's Phi-3 Mini 4K Instruct, to translate English into Yoda-speak. You can think of this initial chapters as a recipe you can just follow. It is a \"shoot first, ask questions later\" kind of chapter.\n",
        "\n",
        "You'll learn how to:\n",
        "- load a quantized model using `BitsAndBytes`\n",
        "- configure low-rank adapters (LoRA) using Hugging Face's `peft`\n",
        "- load and format a dataset\n",
        "- fine-tune the model using the supervised fine-tuning trainer (`SFTTrainer`) from Hugging Face's `trl`\n",
        "- use the fine-tuned model to generate a few sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cb88c3d",
      "metadata": {
        "id": "5cb88c3d"
      },
      "source": [
        "### Setup\n",
        "\n",
        "For better reproducibility during training, use the pinned versions below, the same versions used in the book:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a165c97",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a165c97",
        "outputId": "c2f42b9e-0125-4f19-9aae-5343ae5764a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[Kâ£Ÿ installing transformers from spec 'transformers==4.46.2'â£» installing transformers from spec 'transformers==4.46.2'â¢¿ installing transformers from spec 'transformers==4.46.2'  installed package \u001b[1mtransformers\u001b[0m \u001b[1m4.46.2\u001b[0m, installed using Python 3.13.5\n",
            "  These apps are now globally available\n",
            "    - transformers-cli\n",
            "done! âœ¨ ðŸŒŸ âœ¨\n",
            "\u001b[Kâ£» installing peft from spec 'peft==0.13.2'"
          ]
        }
      ],
      "source": [
        "# bitsandbytes had to be bumped to 0.45.2 to avoid errors in Colab env\n",
        "!pipx install transformers==4.46.2 peft==0.13.2 accelerate==1.1.1 trl==0.12.1 bitsandbytes==0.45.2 datasets==3.1.0 huggingface-hub==0.26.2 safetensors==0.4.5 pandas==2.2.2 matplotlib==3.8.0 numpy==1.26.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fd8c357f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd8c357f",
        "outputId": "24e4b305-1001-4e32-cfeb-21c76331b822"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (3.1.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.45.2)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets) (3.12.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from trl) (1.1.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from trl) (13.9.4)\n",
            "Requirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.46.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.0->trl) (5.9.5)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.0->trl) (0.4.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0->trl) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0->trl) (0.20.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->trl) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->trl) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# If you're running on Colab\n",
        "!pip install datasets bitsandbytes trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "def287c0",
      "metadata": {
        "id": "def287c0"
      },
      "outputs": [],
      "source": [
        "# If you're running on runpod.io's Jupyter Template\n",
        "#!pip install datasets bitsandbytes trl transformers peft huggingface-hub accelerate safetensors pandas matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ClzdV38bq5X5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ClzdV38bq5X5",
        "outputId": "f2bcbee2-fa0c-4659-b5b3-a525723f7ac0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Using cached datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting bitsandbytes\n",
            "  Using cached bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
            "Collecting trl\n",
            "  Using cached trl-0.21.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.55.4-py3-none-any.whl.metadata (41 kB)\n",
            "Collecting peft\n",
            "  Using cached peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting huggingface-hub\n",
            "  Using cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting accelerate\n",
            "  Using cached accelerate-1.10.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting safetensors\n",
            "  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting pandas\n",
            "  Using cached pandas-2.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
            "Collecting matplotlib\n",
            "  Using cached matplotlib-3.10.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Using cached datasets-4.0.0-py3-none-any.whl (494 kB)\n",
            "Using cached bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
            "Using cached trl-0.21.0-py3-none-any.whl (511 kB)\n",
            "Using cached transformers-4.55.4-py3-none-any.whl (11.3 MB)\n",
            "Using cached peft-0.17.1-py3-none-any.whl (504 kB)\n",
            "Using cached huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
            "Using cached accelerate-1.10.0-py3-none-any.whl (374 kB)\n",
            "Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
            "Using cached pandas-2.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "Using cached matplotlib-3.10.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "Installing collected packages: trl, transformers, safetensors, peft, pandas, matplotlib, huggingface-hub, datasets, bitsandbytes, accelerate\n",
            "  Attempting uninstall: trl\n",
            "    Found existing installation: trl 0.21.0\n",
            "    Uninstalling trl-0.21.0:\n",
            "      Successfully uninstalled trl-0.21.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.55.4\n",
            "    Uninstalling transformers-4.55.4:\n",
            "      Successfully uninstalled transformers-4.55.4\n",
            "  Attempting uninstall: safetensors\n",
            "    Found existing installation: safetensors 0.6.2\n",
            "    Uninstalling safetensors-0.6.2:\n",
            "      Successfully uninstalled safetensors-0.6.2\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.17.1\n",
            "    Uninstalling peft-0.17.1:\n",
            "      Successfully uninstalled peft-0.17.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.3.2\n",
            "    Uninstalling pandas-2.3.2:\n",
            "      Successfully uninstalled pandas-2.3.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.5\n",
            "    Uninstalling matplotlib-3.10.5:\n",
            "      Successfully uninstalled matplotlib-3.10.5\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.34.4\n",
            "    Uninstalling huggingface-hub-0.34.4:\n",
            "      Successfully uninstalled huggingface-hub-0.34.4\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "  Attempting uninstall: bitsandbytes\n",
            "    Found existing installation: bitsandbytes 0.47.0\n",
            "    Uninstalling bitsandbytes-0.47.0:\n",
            "      Successfully uninstalled bitsandbytes-0.47.0\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.10.0\n",
            "    Uninstalling accelerate-1.10.0:\n",
            "      Successfully uninstalled accelerate-1.10.0\n",
            "Successfully installed accelerate-1.10.0 bitsandbytes-0.47.0 datasets-4.0.0 huggingface-hub-0.34.4 matplotlib-3.10.5 pandas-2.3.2 peft-0.17.1 safetensors-0.6.2 transformers-4.55.4 trl-0.21.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "db90d31b4c054825ab1a4972cc517883",
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "# Reinstalling packages with --no-deps to avoid conflicts, then install numpy\n",
        "!pip install --upgrade --force-reinstall --no-deps datasets bitsandbytes trl transformers peft huggingface-hub accelerate safetensors pandas matplotlib\n",
        "!pip install numpy==1.26.4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab94a803",
      "metadata": {
        "id": "ab94a803"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b9d45274",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "b9d45274",
        "outputId": "5a28d4cb-cee7-43ec-a640-ce98f9836c9f",
        "scrolled": true
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Only a single TORCH_LIBRARY can be used to register the namespace triton; please put all of your definitions in a single TORCH_LIBRARY block.  If you were trying to specify implementations, consider using TORCH_LIBRARY_IMPL (which can be duplicated).  If you really intended to define operators for a single namespace in a distributed way, you can use TORCH_LIBRARY_FRAGMENT to explicitly indicate this.  Previous registration of TORCH_LIBRARY was registered at /dev/null:2687; latest registration was registered at /dev/null:2687",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-42532071.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_peft_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoraConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepare_model_for_kbit_training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBitsAndBytesConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2685\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompiler\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcompiler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2687\u001b[0;31m     \u001b[0;32mclass\u001b[0m \u001b[0m_TritonLibrary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2688\u001b[0m         \u001b[0mlib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"triton\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DEF\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2689\u001b[0m         \u001b[0mops_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Callable\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m_TritonLibrary\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0m_TritonLibrary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2688\u001b[0;31m         \u001b[0mlib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"triton\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DEF\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2689\u001b[0m         \u001b[0mops_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Callable\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/library.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ns, kind, dispatch_key)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         self.m: Optional[Any] = torch._C._dispatch_library(\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdispatch_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Only a single TORCH_LIBRARY can be used to register the namespace triton; please put all of your definitions in a single TORCH_LIBRARY block.  If you were trying to specify implementations, consider using TORCH_LIBRARY_IMPL (which can be duplicated).  If you really intended to define operators for a single namespace in a distributed way, you can use TORCH_LIBRARY_FRAGMENT to explicitly indicate this.  Previous registration of TORCH_LIBRARY was registered at /dev/null:2687; latest registration was registered at /dev/null:2687"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from trl import SFTConfig, SFTTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a64ffab5",
      "metadata": {
        "id": "a64ffab5"
      },
      "source": [
        "## Loading a Quantized Base Model\n",
        "\n",
        "We start by loading a quantized model, so it takes up less space in the GPU's RAM. A quantized model replaces the original weights with approximate values that are represented by fewer bits. The simplest and most straightforward way to quantize a model is to turn its weights from 32-bit floating-point (FP32) numbers into 4-bit floating-point numbers (NF4). This simple yet powerful change already **reduces the model's memory footprint** by roughly a factor of eight.\n",
        "\n",
        "We can use an instance of `BitsAndBytesConfig` as the `quantization_config` argument while loading a model using the `from_pretrained()` method. To keep it flexible, so you can try it out with any other model of your choice, we're using Hugging Face's\n",
        "`AutoModelForCausalLM`. The repo you choose to use determines the model being loaded.\n",
        "\n",
        "Without further ado, here's our quantized model being loaded:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5ad7668",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "91565e29ebc34b759dfb5710c4c1cb02"
          ]
        },
        "id": "f5ad7668",
        "outputId": "e0bc0d27-6502-4148-cf5a-0763617f0ccf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91565e29ebc34b759dfb5710c4c1cb02",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "   load_in_4bit=True,\n",
        "   bnb_4bit_quant_type=\"nf4\",\n",
        "   bnb_4bit_use_double_quant=True,\n",
        "   bnb_4bit_compute_dtype=torch.float32\n",
        ")\n",
        "repo_id = 'microsoft/Phi-3-mini-4k-instruct'\n",
        "model = AutoModelForCausalLM.from_pretrained(repo_id,\n",
        "                                             device_map=\"cuda:0\",\n",
        "                                             quantization_config=bnb_config\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f26f487f",
      "metadata": {
        "id": "f26f487f"
      },
      "source": [
        "<blockquote class=\"note\">\n",
        "  <p>\n",
        "    <em>\"The Phi-3-Mini-4K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties. The model belongs to the Phi-3 family with the Mini version in two variants 4K and 128K which is the context length (in tokens) that it can support.\"</em>\n",
        "    <br>\n",
        "    Source: <a href=\"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\">Hugging Face Hub</a>\n",
        "  </p>\n",
        "</blockquote>\n",
        "\n",
        "Once the model is loaded, you can see how much space it occupies in memory using the `get_memory_footprint()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b3ac411",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b3ac411",
        "outputId": "3a3c1971-6677-4606-d8e2-9fe8c3b4a22e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2206.347264\n"
          ]
        }
      ],
      "source": [
        "print(model.get_memory_footprint()/1e6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24f09ff7",
      "metadata": {
        "id": "24f09ff7"
      },
      "source": [
        "Even though it's been quantized, the model still takes up a bit more than 2 gigabytes of RAM. The **quantization** procedure focuses on the **linear layers within the Transformer decoder blocks** (also referred to as \"layers\" in some cases):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdc64043",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdc64043",
        "outputId": "cbc3114f-2609-4c02-e429-8f4ddc1a2ad5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Phi3ForCausalLM(\n",
              "  (model): Phi3Model(\n",
              "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
              "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x Phi3DecoderLayer(\n",
              "        (self_attn): Phi3Attention(\n",
              "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
              "          (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
              "          (rotary_emb): Phi3RotaryEmbedding()\n",
              "        )\n",
              "        (mlp): Phi3MLP(\n",
              "          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
              "          (activation_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
              "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1461b229",
      "metadata": {
        "id": "1461b229"
      },
      "source": [
        "A **quantized model** can be used directly for inference, but it **cannot be trained any further**. Those pesky `Linear4bit` layers take up much less space, which is the whole point of quantization; however, we cannot update them.\n",
        "\n",
        "We need to add something else to our mix, a sprinkle of adapters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f9e6061",
      "metadata": {
        "id": "0f9e6061"
      },
      "source": [
        "## Setting Up Low-Rank Adapters (LoRA)\n",
        "\n",
        "Low-rank adapters can be attached to each and every one of the quantized layers. The **adapters** are mostly **regular `Linear`  layers** that can be easily updated as usual. The clever trick in this case is that these adapters are significantly **smaller** than the layers that have been quantized.\n",
        "\n",
        "Since the **quantized layers are frozen** (they cannot be updated), setting up **LoRA adapters** on a quantized model drastically **reduces the total number of trainable parameters** to just 1% (or less) of its original size.\n",
        "\n",
        "We can set up LoRA adapters in three easy steps:\n",
        "\n",
        "* Call `prepare_model_for_kbit_training()` to _improve numerical stability_ during training.\n",
        "* Create an instance of `LoraConfig`.\n",
        "* Apply the configuration to the quantized base model using the `get_peft_model()` method.\n",
        "\n",
        "Let's try it out with our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3edc24ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3edc24ed",
        "outputId": "af1841dc-2845-4625-99a7-c57a8f11670d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): Phi3ForCausalLM(\n",
              "      (model): Phi3Model(\n",
              "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
              "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x Phi3DecoderLayer(\n",
              "            (self_attn): Phi3Attention(\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (qkv_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=9216, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): Phi3RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Phi3MLP(\n",
              "              (gate_up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=16384, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (activation_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
              "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,                   # the rank of the adapter, the lower the fewer parameters you'll need to train\n",
        "    lora_alpha=16,         # multiplier, usually 2*r\n",
        "    bias=\"none\",           # BEWARE: training biases *modifies* base model's behavior\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    # Newer models, such as Phi-3 at time of writing, may require\n",
        "    # manually setting target modules\n",
        "    target_modules=['o_proj', 'qkv_proj', 'gate_up_proj', 'down_proj'],\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "611f1cf6",
      "metadata": {
        "id": "611f1cf6"
      },
      "source": [
        "The output of the other three LoRA layers (`qkv_proj`, `gate_up_proj`, and `down_proj`) was suppressed to shorten the output.\n",
        "\n",
        "<blockquote class=\"warning\">\n",
        "  <p>\n",
        "    Did you get the following error?\n",
        "    <br>\n",
        "    <br>\n",
        "    <tt>ValueError: Please specify `target_modules` in `peft_config`</tt>\n",
        "    <br>\n",
        "    <br>\n",
        "    Most likely, you don't need to specify the <tt>target_modules</tt> if you're using one of the well-known models. The <tt>peft</tt> library takes care of it by <em>automatically choosing the appropriate targets</em>. However, there may be a gap between the time a popular model is released and the time the library gets updated. So, if you get the error above, look for the quantized layers in your model and list their names in the <tt>target_modules</tt> argument.\n",
        "  </p>\n",
        "</blockquote>\n",
        "\n",
        "The quantized layers (`Linear4bit`) have turned into `lora.Linear4bit` modules where the quantized layer itself became the `base_layer` with some regular `Linear` layers (`lora_A` and `lora_B`) added to the mix.\n",
        "\n",
        "These extra layers would make the model only slightly larger. However, **the model preparation function** (`prepare_model_for_kbit_training()`) turned **every non-quantized layer to full precision (FP32)**, thus resulting in a 30% larger model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f30ecbd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f30ecbd",
        "outputId": "bc701908-71fd-408d-b0f9-cb907083998c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2651.080704\n"
          ]
        }
      ],
      "source": [
        "print(model.get_memory_footprint()/1e6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e2718a6",
      "metadata": {
        "id": "2e2718a6"
      },
      "source": [
        "Since most parameters are frozen, only a tiny fraction of the total number of parameters are currently trainable, thanks to LoRA!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c06c42b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c06c42b4",
        "outputId": "e2fbc843-0b3c-4856-a386-8b03e5a11db0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable parameters:             12.58M\n",
            "Total parameters:                 3833.66M\n",
            "Fraction of trainable parameters: 0.33%\n"
          ]
        }
      ],
      "source": [
        "trainable_parms, tot_parms = model.get_nb_trainable_parameters()\n",
        "print(f'Trainable parameters:             {trainable_parms/1e6:.2f}M')\n",
        "print(f'Total parameters:                 {tot_parms/1e6:.2f}M')\n",
        "print(f'Fraction of trainable parameters: {100*trainable_parms/tot_parms:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "149381b5",
      "metadata": {
        "id": "149381b5"
      },
      "source": [
        "The model is ready to be fine-tuned, but we are still missing one key component: our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2547d258",
      "metadata": {
        "id": "2547d258"
      },
      "source": [
        "## Formatting Your Dataset\n",
        "\n",
        "<blockquote style=\"quotes: none !important;\">\n",
        "  <p>\n",
        "    <em>\"Like Yoda, speak, you must. Hrmmm.\"</em>\n",
        "    <br>\n",
        "    <br>\n",
        "    Master Yoda\n",
        "  </p>\n",
        "</blockquote>\n",
        "\n",
        "The dataset [`yoda_sentences`](https://huggingface.co/datasets/dvgodoy/yoda_sentences) consists of 720 sentences translated from English to Yoda-speak. The dataset is hosted on the Hugging Face Hub and we can easily load it using the `load_dataset()` method from the Hugging Face `datasets` library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3251cca",
      "metadata": {
        "id": "a3251cca",
        "outputId": "7cfd0ed3-175d-4a04-cffc-d63ec9bcb482"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['sentence', 'translation', 'translation_extra'],\n",
              "    num_rows: 720\n",
              "})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1da3b11",
      "metadata": {
        "id": "a1da3b11"
      },
      "source": [
        "The dataset has three columns:\n",
        "\n",
        "* original English sentence (`sentence`)\n",
        "* basic translation to Yoda-speak (`translation`)\n",
        "* enhanced translation including typical `Yesss` and `Hrrmm` interjections (`translation_extra`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c804839",
      "metadata": {
        "id": "2c804839",
        "outputId": "644a8843-fb28-45ab-9ccc-6456d0a2935c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'sentence': 'The birch canoe slid on the smooth planks.',\n",
              " 'translation': 'On the smooth planks, the birch canoe slid.',\n",
              " 'translation_extra': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.'}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37005b36",
      "metadata": {
        "id": "37005b36"
      },
      "source": [
        "The `SFTTrainer` we'll be using to fine-tune the model can automatically handle datasets either in **conversational** or **instruction** formats.\n",
        "\n",
        "* **conversational format**\n",
        "\n",
        "```\n",
        "{\"messages\":[\n",
        "  {\"role\": \"system\", \"content\": \"<general directives>\"},\n",
        "  {\"role\": \"user\", \"content\": \"<prompt text>\"},\n",
        "  {\"role\": \"assistant\", \"content\": \"<ideal generated text>\"}\n",
        "]}\n",
        "```\n",
        "\n",
        "* **instruction format**: **[unfortunately, recent versions of `trl` do not support this format properly anymore, please check the important update a few cells below]**\n",
        "\n",
        "```\n",
        "{\"prompt\": \"<prompt text>\",\n",
        "\"completion\": \"<ideal generated text>\"}\n",
        "```\n",
        "\n",
        "Since the instruction format is easier to work with, we'll simply rename and keep the relevant columns from our dataset. That's it for formatting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "601b15b7",
      "metadata": {
        "id": "601b15b7",
        "outputId": "d866b834-8983-499e-8c0a-e27f4c54c3b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['prompt', 'completion'],\n",
              "    num_rows: 720\n",
              "})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
        "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
        "dataset = dataset.remove_columns([\"translation\"])\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ae9739f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ae9739f",
        "outputId": "7e9d6c7f-6901-492e-cf14-2fd607e11862"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'prompt': 'The birch canoe slid on the smooth planks.',\n",
              " 'completion': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.'}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfdfd798",
      "metadata": {
        "id": "bfdfd798"
      },
      "source": [
        "Internally, the training data will be converted from the instruction to the conversational format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b150ab79",
      "metadata": {
        "id": "b150ab79",
        "outputId": "4d61eaa0-75b1-4151-fbab-ebcdec50625a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'role': 'user', 'content': 'The birch canoe slid on the smooth planks.'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.'}]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": dataset[0]['prompt']},\n",
        "    {\"role\": \"assistant\", \"content\": dataset[0]['completion']}\n",
        "]\n",
        "messages"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45ab6fe3",
      "metadata": {
        "id": "45ab6fe3"
      },
      "source": [
        "***\n",
        "\n",
        "**IMPORTANT UPDATE**: unfortunately, in more recent versions of the `trl` library, the \"instruction\" format is not properly supported anymore, thus leading to the chat template not being applied to the dataset. In order to avoid this issue, we can convert the dataset to the \"conversational\" format.\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eca9e9fa",
      "metadata": {
        "id": "eca9e9fa"
      },
      "outputs": [],
      "source": [
        "# Adapted from trl.extras.dataset_formatting.instructions_formatting_function\n",
        "# Converts dataset from prompt/completion format (not supported anymore)\n",
        "# to the conversational format\n",
        "def format_dataset(examples):\n",
        "    if isinstance(examples[\"prompt\"], list):\n",
        "        output_texts = []\n",
        "        for i in range(len(examples[\"prompt\"])):\n",
        "            converted_sample = [\n",
        "                {\"role\": \"user\", \"content\": examples[\"prompt\"][i]},\n",
        "                {\"role\": \"assistant\", \"content\": examples[\"completion\"][i]},\n",
        "            ]\n",
        "            output_texts.append(converted_sample)\n",
        "        return {'messages': output_texts}\n",
        "    else:\n",
        "        converted_sample = [\n",
        "            {\"role\": \"user\", \"content\": examples[\"prompt\"]},\n",
        "            {\"role\": \"assistant\", \"content\": examples[\"completion\"]},\n",
        "        ]\n",
        "        return {'messages': converted_sample}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbf5c97e",
      "metadata": {
        "id": "cbf5c97e"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.map(format_dataset).remove_columns(['prompt', 'completion'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "288ab1a7",
      "metadata": {
        "id": "288ab1a7"
      },
      "source": [
        "### Tokenizer\n",
        "\n",
        "Before moving into the actual training, we still need to **load the tokenizer that corresponds to our model**. The tokenizer is an important part of this process, determining how to convert text into tokens in the same way used to train the model.\n",
        "\n",
        "For instruction/chat models, the tokenizer also contains its corresponding **chat template** that specifies:\n",
        "\n",
        "* Which **special tokens** should be used, and where they should be placed.\n",
        "* Where the system directives, user prompt, and model response should be placed.\n",
        "* What is the **generation prompt**, that is, the special token that triggers the model's response (more on that in the \"Querying the Model\" section)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a647f985",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a647f985",
        "outputId": "e312eb2b-02d1-4466-af1b-858a57a221af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'user' %}{{'<|user|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\\n' + message['content'] + '<|end|>\\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\\n' }}{% else %}{{ eos_token }}{% endif %}\""
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
        "tokenizer.chat_template"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b4f7d28",
      "metadata": {
        "id": "1b4f7d28"
      },
      "source": [
        "Never mind the seemingly overcomplicated template (I have added line breaks and indentation to it so it's easier to read). It simply organizes the messages into a coherent block with the appropriate tags, as shown below (`tokenize=False` ensures we get readable text back instead of a numeric sequence of token IDs):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b2cbfd6",
      "metadata": {
        "id": "2b2cbfd6",
        "outputId": "9a4943c3-aecc-42c1-d157-897dd72ee33b",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|user|>\n",
            "The birch canoe slid on the smooth planks.<|end|>\n",
            "<|assistant|>\n",
            "On the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|>\n",
            "<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.apply_chat_template(messages, tokenize=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a44fd5c6",
      "metadata": {
        "id": "a44fd5c6"
      },
      "source": [
        "Notice that each interaction is wrapped in either `<|user|>` or `<|assistant|>` tokens at the beginning and `<|end|>` at the end. Moreover, the `<|endoftext|>` token indicates the end of the whole block.\n",
        "\n",
        "Different models will have different templates and tokens to indicate the beginning and end of sentences and blocks.\n",
        "\n",
        "We're now ready to tackle the actual fine-tuning!\n",
        "\n",
        "***\n",
        "**IMPORTANT UPDATE**: due to changes in the default collator used by the `SFTTrainer` class while building the dataset, the EOS token (which is, in Phi-3, the same as the PAD token) was masked in the labels too thus leading to the model not being able to properly stop token generation.\n",
        "\n",
        "In order to address this change, we can assign the UNK token to the PAD token, so the EOS token becomes unique and therefore not masked as part of the labels.\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4af61ed6",
      "metadata": {
        "id": "4af61ed6"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id = tokenizer.unk_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ada15ad",
      "metadata": {
        "id": "4ada15ad"
      },
      "source": [
        "## Fine-Tuning with SFTTrainer\n",
        "\n",
        "**Fine-tuning a model**, whether large or otherwise, follows exactly **the same training procedure as training a model from scratch**. We could write our own training loop in pure PyTorch, or we could use Hugging Face's `Trainer` to fine-tune our model.\n",
        "\n",
        "It is much easier, however, to use `SFTTrainer` instead (which uses `Trainer` underneath, by the way), since it takes care of most of the nitty-gritty details for us, as long as we provide it with the following four arguments:\n",
        "\n",
        "* a model\n",
        "* a tokenizer\n",
        "* a dataset\n",
        "* a configuration object\n",
        "\n",
        "We've already got the first three elements; let's work on the last one.\n",
        "\n",
        "### SFTConfig\n",
        "\n",
        "There are many parameters that we can set in the configuration object. We have divided them into four groups:\n",
        "\n",
        "* **Memory usage** optimization parameters related to **gradient accumulation and checkpointing**\n",
        "* **Dataset**-related arguments, such as the `max_seq_length` required by your data, and whether you are packing or not the sequences\n",
        "* Typical **training parameters** such as the `learning_rate` and the `num_train_epochs`\n",
        "* **Environment and logging** parameters such as `output_dir` (this will be the name of the model if you choose to push it to the Hugging Face Hub once it's trained), `logging_dir`, and `logging_steps`.\n",
        "\n",
        "While the _learning rate_ is a very important parameter (as a starting point, you can try the learning rate used to train the base model in the first place), it's actually the **maximum sequence length** that's more likely to cause **out-of-memory issues**.\n",
        "\n",
        "Make sure to always pick the shortest possible `max_seq_length` that makes sense for your use case. In ours, the sentencesâ€”both in English and Yoda-speakâ€”are quite short, and a sequence of 64 tokens is more than enough to cover the prompt, the completion, and the added special tokens.\n",
        "\n",
        "<blockquote class=\"tip\">\n",
        "  <p>\n",
        "    Flash attention (which, unfortunately, isn't supported in Colab), allows for more flexibility in working with longer sequences, avoiding the potential issue of OOM errors.\n",
        "  </p>\n",
        "</blockquote>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a9979b2",
      "metadata": {
        "id": "7a9979b2"
      },
      "outputs": [],
      "source": [
        "sft_config = SFTConfig(\n",
        "    ## GROUP 1: Memory usage\n",
        "    # These arguments will squeeze the most out of your GPU's RAM\n",
        "    # Checkpointing\n",
        "    gradient_checkpointing=True,\n",
        "    # this saves a LOT of memory\n",
        "    # Set this to avoid exceptions in newer versions of PyTorch\n",
        "    gradient_checkpointing_kwargs={'use_reentrant': False},\n",
        "    # Gradient Accumulation / Batch size\n",
        "    # Actual batch (for updating) is same (1x) as micro-batch size\n",
        "    gradient_accumulation_steps=1,\n",
        "    # The initial (micro) batch size to start off with\n",
        "    per_device_train_batch_size=16,\n",
        "    # If batch size would cause OOM, halves its size until it works\n",
        "    auto_find_batch_size=True,\n",
        "\n",
        "    ## GROUP 2: Dataset-related\n",
        "    max_seq_length=64,\n",
        "    # Dataset\n",
        "    # packing a dataset means no padding is needed\n",
        "    packing=True,\n",
        "\n",
        "    ## GROUP 3: These are typical training parameters\n",
        "    num_train_epochs=10,\n",
        "    learning_rate=3e-4,\n",
        "    # Optimizer\n",
        "    # 8-bit Adam optimizer - doesn't help much if you're using LoRA!\n",
        "    optim='paged_adamw_8bit',\n",
        "\n",
        "    ## GROUP 4: Logging parameters\n",
        "    logging_steps=10,\n",
        "    logging_dir='./logs',\n",
        "    output_dir='./phi3-mini-yoda-adapter',\n",
        "    report_to='none'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "971e3099",
      "metadata": {
        "id": "971e3099"
      },
      "source": [
        "### `SFTTrainer`\n",
        "\n",
        "<blockquote style=\"quotes: none !important;\">\n",
        "  <p>\n",
        "    <em>\"It is training time!\"</em>\n",
        "    <br>\n",
        "    <br>\n",
        "    The Hulk\n",
        "  </p>\n",
        "</blockquote>\n",
        "\n",
        "We can now finally create an instance of the supervised fine-tuning trainer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f194a639",
      "metadata": {
        "id": "f194a639"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    args=sft_config,\n",
        "    train_dataset=dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "105c4efd",
      "metadata": {
        "id": "105c4efd"
      },
      "source": [
        "The `SFTTrainer` had already preprocessed our dataset, so we can take a look inside and see how each mini-batch was assembled:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ffc0403",
      "metadata": {
        "id": "1ffc0403"
      },
      "outputs": [],
      "source": [
        "dl = trainer.get_train_dataloader()\n",
        "batch = next(iter(dl))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfaf09e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfaf09e2",
        "outputId": "9be6fe5d-2202-4733-cbbe-74c47251d307"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([13283, 11182, 29892,   278,   868,   446,  1258, 29889, 32007, 32000,\n",
              "         32000, 32010,  4122,   300,   278,   716, 28865,   322,  5967,  9098,\n",
              "         29889, 32007, 32001,  4122,   300,   278,   716, 28865,   322,  5967,\n",
              "          9098, 29892,   366,  1818, 29889, 28756, 29889, 32007, 32000, 32000,\n",
              "         32010,  1932,   278, 14671,   303,   756,  2041,   372,   338,   931,\n",
              "           363,  7013,  1989, 29889, 32007, 32001, 26399,  1758,  4317, 29889,\n",
              "          5974,   363,  7013,  1989], device='cuda:0'),\n",
              " tensor([13283, 11182, 29892,   278,   868,   446,  1258, 29889, 32007, 32000,\n",
              "         32000, 32010,  4122,   300,   278,   716, 28865,   322,  5967,  9098,\n",
              "         29889, 32007, 32001,  4122,   300,   278,   716, 28865,   322,  5967,\n",
              "          9098, 29892,   366,  1818, 29889, 28756, 29889, 32007, 32000, 32000,\n",
              "         32010,  1932,   278, 14671,   303,   756,  2041,   372,   338,   931,\n",
              "           363,  7013,  1989, 29889, 32007, 32001, 26399,  1758,  4317, 29889,\n",
              "          5974,   363,  7013,  1989], device='cuda:0'))"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch['input_ids'][0], batch['labels'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eedef238",
      "metadata": {
        "id": "eedef238"
      },
      "source": [
        "The **labels were added automatically**, and they're **exactly the same as the inputs**. Thus, this is a case of **self-supervised fine-tuning**.\n",
        "\n",
        "The shifting of the labels will be handled automatically as well; there's no need to be concerned about it.\n",
        "\n",
        "<blockquote class=\"note\">\n",
        "  <p>\n",
        "    Although this is a 3.8 billion-parameter model, the configuration above allows us to squeeze training, using a mini-batch of eight, into an old setup with a consumer-grade GPU such as a GTX 1060 with only 6 GB RAM. True story!\n",
        "    <br>\n",
        "    It takes about 35 minutes to complete the training process.\n",
        "  </p>\n",
        "</blockquote>\n",
        "\n",
        "Next, we call the `train()` method and wait:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b12a099d",
      "metadata": {
        "id": "b12a099d",
        "outputId": "c33c3586-8cd6-4902-88a9-74f3d6c4bf33"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='220' max='220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [220/220 34:02, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.990700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.789500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.581700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.458300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.362300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.264100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.159600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.943600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.847900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.607900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.588200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.415300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.428000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.369100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.353600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.334000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.306700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.293400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.275300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.277500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.253800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.252400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=220, training_loss=0.8251331968740984, metrics={'train_runtime': 2051.962, 'train_samples_per_second': 1.711, 'train_steps_per_second': 0.107, 'total_flos': 5034400555991040.0, 'train_loss': 0.8251331968740984, 'epoch': 10.0})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f4b666a",
      "metadata": {
        "id": "2f4b666a"
      },
      "source": [
        "## Querying the Model\n",
        "\n",
        "Now, our model should be able to produce a Yoda-like sentence as a response to any short sentence we give it.\n",
        "\n",
        "So, the model requires its inputs to be properly formatted. We need to build a list of \"messages\"â€”ours, from the `user`, in this caseâ€”and prompt the model to answer by indicating it's its turn to write.\n",
        "\n",
        "This is the purpose of the `add_generation_prompt` argument: it adds `<|assistant|>` to the end of the conversation, so the model can predict the next wordâ€”and continue doing so until it predicts an `<|endoftext|>` token.\n",
        "\n",
        "The helper function below assembles a message (in the conversational format) and **applies the chat template** to it, **appending the generation prompt** to its end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5854efcd",
      "metadata": {
        "id": "5854efcd"
      },
      "outputs": [],
      "source": [
        "def gen_prompt(tokenizer, sentence):\n",
        "    converted_sample = [\n",
        "        {\"role\": \"user\", \"content\": sentence},\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(converted_sample,\n",
        "                                           tokenize=False,\n",
        "                                           add_generation_prompt=True)\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "438ba48b",
      "metadata": {
        "id": "438ba48b"
      },
      "source": [
        "Let's try generating a prompt for an example sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "480a479b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "480a479b",
        "outputId": "02b31057-a910-45fc-ca5b-a82800d56528"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|user|>\n",
            "The Force is strong in you!<|end|>\n",
            "<|assistant|>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sentence = 'The Force is strong in you!'\n",
        "prompt = gen_prompt(tokenizer, sentence)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4773df8d",
      "metadata": {
        "id": "4773df8d"
      },
      "source": [
        "The prompt seems about right; let's use it to generate a completion. The helper function below does the following:\n",
        "\n",
        "* It **tokenizes the prompt** into a tensor of token IDs (`add_special_tokens` is set to `False` because the tokens were already added by the chat template).\n",
        "* It sets the model to **evaluation mode**.\n",
        "* It calls the model's `generate()` method to **produce the output** (generated token IDs).\n",
        "* It **decodes the generated token IDs** back into readable text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65a53ea9",
      "metadata": {
        "id": "65a53ea9"
      },
      "outputs": [],
      "source": [
        "def generate(model, tokenizer, prompt, max_new_tokens=64, skip_special_tokens=False):\n",
        "    tokenized_input = tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    model.eval()\n",
        "    generation_output = model.generate(**tokenized_input,\n",
        "                                       eos_token_id=tokenizer.eos_token_id,\n",
        "                                       max_new_tokens=max_new_tokens)\n",
        "\n",
        "    output = tokenizer.batch_decode(generation_output,\n",
        "                                    skip_special_tokens=skip_special_tokens)\n",
        "    return output[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1f6b3bf",
      "metadata": {
        "id": "e1f6b3bf"
      },
      "source": [
        "Now, we can finally try out our model and see if it's indeed capable of generating Yoda-speak."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33dca746",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33dca746",
        "outputId": "eb10cbe8-0025-47ab-ce05-9364f3208323"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|user|> The Force is strong in you!<|end|><|assistant|> Strong in you, the Force is!<|end|>\n"
          ]
        }
      ],
      "source": [
        "print(generate(model, tokenizer, prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56e821c3",
      "metadata": {
        "id": "56e821c3"
      },
      "source": [
        "Awesome! It works! Like Yoda, the model speaks. Hrrrmm.\n",
        "\n",
        "Congratulations, you've fine-tuned your first LLM!\n",
        "\n",
        "Now, you've got a small adapter that can be loaded into an instance of the Phi-3 Mini 4K Instruct model to turn it into a Yoda translator! How cool is that?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "898dc809",
      "metadata": {
        "id": "898dc809"
      },
      "source": [
        "### Saving the Adapter\n",
        "\n",
        "Once the training is completed, you can save the adapter (and the tokenizer) to disk by calling the trainer's `save_model()` method. It will save everything to the specified folder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b41a86d2",
      "metadata": {
        "id": "b41a86d2"
      },
      "outputs": [],
      "source": [
        "trainer.save_model('local-phi3-mini-yoda-adapter')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaae1c2d",
      "metadata": {
        "id": "aaae1c2d"
      },
      "source": [
        "The files that were saved include:\n",
        "\n",
        "* the  adapter configuration (`adapter_config.json`) and weights (`adapter_model.safetensors`)â€”the adapter itself is just 50 MB in size\n",
        "* the training arguments (`training_args.bin`)\n",
        "* the tokenizer (`tokenizer.json` and `tokenizer.model`), its configuration (`tokenizer_config.json`), and its special tokens (`added_tokens.json` and `speciak_tokens_map.json`)\n",
        "* a README file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "761d1ba3",
      "metadata": {
        "id": "761d1ba3",
        "outputId": "f02751cb-dd2d-4cfb-cd4a-6a7165e7dedf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['tokenizer.json',\n",
              " 'README.md',\n",
              " 'training_args.bin',\n",
              " 'adapter_config.json',\n",
              " 'special_tokens_map.json',\n",
              " 'added_tokens.json',\n",
              " 'tokenizer_config.json',\n",
              " 'adapter_model.safetensors',\n",
              " 'tokenizer.model']"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.listdir('local-phi3-mini-yoda-adapter')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "014134ea",
      "metadata": {
        "id": "014134ea"
      },
      "source": [
        "If you'd like to share your adapter with everyone, you can also push it to the Hugging Face Hub. First, log in using a token that has permission to write:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33c1005a",
      "metadata": {
        "id": "33c1005a"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38c5de92",
      "metadata": {
        "id": "38c5de92"
      },
      "source": [
        "The code above will ask you to enter an access token:\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch0/hub0.png?raw=True)\n",
        "<center>Figure 0.1 - Logging into the Hugging Face Hub</center>\n",
        "\n",
        "A successful login should look like this (pay attention to the permissions):\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch0/hub1.png?raw=True)\n",
        "<center>Figure 0.2 - Successful Login</center>\n",
        "\n",
        "Then, you can use the trainer's `push_to_hub()` method to upload everything to your account in the Hub. The model will be named after the `output_dir` argument of the training arguments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e11e931a",
      "metadata": {
        "id": "e11e931a"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5ca8a42",
      "metadata": {
        "id": "a5ca8a42"
      },
      "source": [
        "There you go! Our model is out there in the world, and anyone can use it to translate English into Yoda speak.\n",
        "\n",
        "That's a wrap!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
