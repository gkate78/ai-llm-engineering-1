{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fa_QpI0RXQKx"
   },
   "source": [
    "# LangSmith and Evaluation Overview with AI Makerspace\n",
    "\n",
    "Today we'll be looking at an amazing tool:\n",
    "\n",
    "[LangSmith](https://docs.smith.langchain.com/)!\n",
    "\n",
    "This tool will help us monitor, test, debug, and evaluate our LangChain applications - and more!\n",
    "\n",
    "We'll also be looking at a few Advanced Retrieval techniques along the way - and evaluate it using LangSmith!\n",
    "\n",
    "✋BREAKOUT ROOM #2:\n",
    "- Task 1: Dependencies and OpenAI API Key\n",
    "- Task 2: LangGraph RAG\n",
    "- Task 3: Setting Up LangSmith\n",
    "- Task 4: Examining the Trace in LangSmith!\n",
    "- Task 5: Create Testing Dataset\n",
    "- Task 6: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tw5ok9p-XuUs"
   },
   "source": [
    "## Task 1: Dependencies and OpenAI API Key\n",
    "\n",
    "We'll be using OpenAI's suite of models today to help us generate and embed our documents for our simple RAG system that leverages Loand Complaint data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ADl8-whIAUHD",
    "outputId": "927fd78b-8510-4bea-9e68-a3c74d3eb5a1"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your OpenAI API Key: ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "923xinz42sWV"
   },
   "source": [
    "#### Asyncio Bug Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "m9U0SbQN2sWc"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cx2AOb-QHwJm"
   },
   "source": [
    "## Task #2: Create a Simple RAG Application Using LangGraph\n",
    "\n",
    "Let's remake our LangGraph RAG pipeline from the first notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYJB3IxEwpnh"
   },
   "source": [
    "## LangGraph Powered RAG\n",
    "\n",
    "First and foremost, LangChain provides a convenient way to store our chunks and their embeddings.\n",
    "\n",
    "It's called a `VectorStore`!\n",
    "\n",
    "We'll be using QDrant as our `VectorStore` today. You can read more about it [here](https://qdrant.tech/documentation/).\n",
    "\n",
    "Think of a `VectorStore` as a smart way to house your chunks and their associated embedding vectors. The implementation of the `VectorStore` also allows for smarter and more efficient search of our embedding vectors - as the method we used above would not scale well as we got into the millions of chunks.\n",
    "\n",
    "Otherwise, the process remains relatively similar under the hood!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBErqPRgxgZR"
   },
   "source": [
    "### Data Collection\n",
    "\n",
    "We'll be leveraging the `DirectoryLoader` to load our PDFs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-AHA9L3Jxo3r",
    "outputId": "c3535941-ce13-4ddd-ef08-b6b4ab1e507b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“Then destroy them!”\\xa0rejoined Simoun coldly.  \\n“And the money to pay the laborers?”  \\n“Don’t pay them! Use the prisoners and convicts!”  \\n“But there aren’t\\xa0enough,\\xa0Señor\\xa0Simoun!”  \\n“Then, if there aren’t enough, let all the villagers, the old men, the youths, the boys, work. Instead of the fifteen days \\nof obligatory service, let them work three, four, five months for the State, with the additional obligation that each one \\nprovide his own food and tools.”  \\nThe startled Don Custodio turned his head to see if there was any Indian within ear-shot, but fortunately those \\nnearby were rustics, and the two helmsmen seemed to be very much occupied with the windings of the river.  \\n“But,\\xa0Señor\\xa0Simoun—”  \\n“Don’t fool yourself, Don Custodio,”\\xa0continued Simoun dryly, “only in this way are great enterprises carried out \\nwith small means. Thus were constructed the Pyramids, Lake Moeris, and the Colosseum in Rome. Entire provinces \\ncame in from the desert, bringing their tubers to feed on. Old men, youths, and boys labored in transporting stones, \\nhewing them, and carrying them on their shoulders under the direction of the official lash, and afterwards, the \\nsurvivors returned to their homes or perished in the sands of the desert. Then came other provinces, then others, \\nsucceeding one another in the work during years. Thus the task was finished, and now we admire them, we travel, \\nwe go to Egypt and to Home, we extol the Pharaohs and the Antonines. Don’t fool yourself—the dead remain \\ndead, and might only is considered right by posterity.”  \\n“But,\\xa0Señor\\xa0Simoun,\\xa0such\\xa0measures\\xa0might\\xa0provoke\\xa0uprisings,”\\xa0objected Don Custodio, rather uneasy over the turn \\nthe affair had taken.  \\n“Uprisings, ha, ha! Did the Egyptian people ever rebel, I wonder? Did the Jewish prisoners rebel against the pious \\nTitus? Man, I thought you were better informed in history!”  \\nClearly Simoun was either very presumptuous or disregarded conventionalities! To say to Don Custodio’s face that \\nhe did not know history! It was enough to make any one lose his temper! So it seemed, for Don Custodio forgot \\nhimself and retorted, “But the fact is that you’re not among Egyptians or Jews!”  \\n“And these people have rebelled more than once,”\\xa0added the Dominican, somewhat timidly. “In the times when they \\nwere forced to transport heavy timbers for the construction of ships, if it hadn’t been for the clerics—”  \\n“Those times are far away,”\\xa0answered Simoun, with a laugh even drier than usual. “These islands will never again \\nrebel, no matter how much work and taxes they have. Haven’t you lauded to me, Padre Salvi,”\\xa0he added, turning to \\nthe Franciscan, “the\\xa0house\\xa0and\\xa0hospital\\xa0at\\xa0Los\\xa0Baños,\\xa0where\\xa0his\\xa0Excellency\\xa0is\\xa0at\\xa0present?”  \\nPadre Salvi gave a nod and looked up, evading the question.  \\n“Well, didn’t you tell me that both buildings were constructed by forcing the people to work on them under the whip \\nof a lay-brother? Perhaps that wonderful bridge was built in the same way. Now tell me, did these people rebel?”  \\n“The fact is—they have rebelled before,”\\xa0replied the Dominican, “and ab actu ad posse valet illatio!”  \\n“No, no, nothing of the kind,”\\xa0continued Simoun, starting down a hatchway to the cabin. “What’s said, is said! And \\nyou, Padre Sibyla, don’t talk either Latin or nonsense. What are you friars good for if the people can rebel?”  \\nTaking no notice of the replies and protests, Simoun descended the small companionway that led below, repeating \\ndisdainfully, “Bosh, bosh!”  \\nPadre Sibyla turned pale; this was the first time that he, Vice-Rector of the University, had ever been credited with \\nnonsense. Don Custodio turned green; at no meeting in which he had ever found himself had he encountered such an \\nadversary.  \\n“An American mulatto!”\\xa0he fumed.  \\n“A British Indian,”\\xa0observed Ben-Zayb in a low tone.  \\n“An American, I tell you, and shouldn’t I know?”\\xa0retorted Don Custodio in ill-humor. “His Excellency has told me \\nso. He’s a jeweler whom the latter knew in Havana, and, as I suspect, the one who got him advancement by lending \\nhim money. So to repay him he has had him come here to let him have a chance and increase his fortune by selling \\ndiamonds—imitations, who knows? And he so ungrateful, that, after getting money from the Indians, he wishes—\\nhuh!”\\xa0The sentence was concluded by a significant wave of the hand.  \\nNo one dared to join in this diatribe. Don Custodio could discredit himself with his Excellency, if he wished, but \\nneither Ben-Zayb, nor Padre Irene, nor Padre Salvi, nor the offended Padre Sibyla had any confidence in the \\ndiscretion of the others.  \\n“The fact is that this man, being an American, thinks no doubt that we are dealing with the redskins. To talk of these \\nmatters on a steamer! Compel, force the people! And he’s the very person who advised the expedition to the \\n[9]\\n[10]\\n[11]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "directory_loader = DirectoryLoader(\"data\", glob=\"**/*.pdf\", loader_cls=PyMuPDFLoader)\n",
    "\n",
    "jr_document = directory_loader.load()\n",
    "\n",
    "jr_document[5].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uH7ZPVJLx6Cn"
   },
   "source": [
    "### Chunking Our Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsSCRQUSyBKT"
   },
   "source": [
    "Let's do the same process as we did before with our `RecursiveCharacterTextSplitter` - but this time we'll use ~200 tokens as our max chunk size!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SzolG1FLx2f_"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def tiktoken_len(text):\n",
    "    tokens = tiktoken.encoding_for_model(\"gpt-4o\").encode(\n",
    "        text,\n",
    "    )\n",
    "    return len(tokens)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 750,\n",
    "    chunk_overlap = 0,\n",
    "    length_function = tiktoken_len,\n",
    ")\n",
    "jose_rizal_chunks = text_splitter.split_documents(jr_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fpV4f1eXyXVJ",
    "outputId": "a666d551-6b20-4761-82ad-6f9b524d1660"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "812"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jose_rizal_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTJ60Ck6ybe_"
   },
   "source": [
    "Let's verify the process worked as intended by checking our max document length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "950mB338yZR8",
    "outputId": "9813bfb9-cf80-4787-c962-69a5b1ccdff6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "722\n"
     ]
    }
   ],
   "source": [
    "max_chunk_length = 0\n",
    "\n",
    "for chunk in jose_rizal_chunks:\n",
    "  max_chunk_length = max(max_chunk_length, tiktoken_len(chunk.page_content))\n",
    "\n",
    "print(max_chunk_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDt3RufQy1cP"
   },
   "source": [
    "Perfect! Now we can carry on to creating and storing our embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kocCe4zLy5qT"
   },
   "source": [
    "### Embeddings and Vector Storage\n",
    "\n",
    "We'll use the `text-embedding-3-small` embedding model again - and `Qdrant` to store all our embedding vectors for easy retrieval later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7M0X1eVlWPFf"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "qdrant_vectorstore = Qdrant.from_documents(\n",
    "    documents=jose_rizal_chunks,\n",
    "    embedding=embedding_model,\n",
    "    location=\":memory:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-NDvjfzXhVp"
   },
   "source": [
    "Now let's set up our retriever, just as we saw before, but this time using LangChain's simple `as_retriever()` method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Edjx19YBXavZ"
   },
   "outputs": [],
   "source": [
    "qdrant_retriever = qdrant_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1OM0DiYcOj-"
   },
   "source": [
    "#### Back to the Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7apXaEBzQai"
   },
   "source": [
    "We're ready to move to the next step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMhcU37dzV5k"
   },
   "source": [
    "### Setting up our RAG\n",
    "\n",
    "We'll use the same LangGraph pipeline we created in the first notebook. \n",
    "\n",
    "Let's think through each part:\n",
    "\n",
    "1. First we need to retrieve context\n",
    "2. We need to pipe that context to our model\n",
    "3. We need to parse that output\n",
    "\n",
    "Let's start by setting up our prompt again, just so it's fresh in our minds!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oatgDa7cOXWV"
   },
   "source": [
    "Complete the prompt so that your RAG application answers queries based on the context provided, but *does not* answer queries if the context is unrelated to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TE5tick_YPJj"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "HUMAN_TEMPLATE = \"\"\"\n",
    "#CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUERY:\n",
    "{query}\n",
    "\n",
    "\"\"\"\n",
    "#Use the provide context to answer the provided user query. Only use the provided context to answer the query. If you do not know the answer, or it's not contained in the provided context respond with \"I don't know\"\n",
    "\n",
    "# \n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", HUMAN_TEMPLATE)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DI2tNXIT1iuB"
   },
   "source": [
    "We'll set our Generator - `gpt-4.1-nano` in this case - below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZ-9gF1x1iEz"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai_chat_model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZKadufhc2RL"
   },
   "source": [
    "#### Our RAG Application\n",
    "\n",
    "Let's spin up the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VnGthXpzzo-R"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "class State(TypedDict):\n",
    "  question: str\n",
    "  context: list[Document]\n",
    "  response: str\n",
    "\n",
    "def retrieve(state: State) -> State:\n",
    "  retrieved_docs = qdrant_retriever.invoke(state[\"question\"])\n",
    "  return {\"context\" : retrieved_docs}\n",
    "\n",
    "def generate(state: State) -> State:\n",
    "  generator_chain = chat_prompt | openai_chat_model | StrOutputParser()\n",
    "  response = generator_chain.invoke({\"query\" : state[\"question\"], \"context\" : state[\"context\"]})\n",
    "  return {\"response\" : response}\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder = graph_builder.add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "rag_graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0KAPrtFMtRd"
   },
   "source": [
    "Let's get a visual understanding of our chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y8ocIXNGMsue",
    "outputId": "eb34fc03-7052-4825-82e1-85489ac84e78"
   },
   "outputs": [],
   "source": [
    "rag_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0C5CFRHOxtB"
   },
   "source": [
    "Let's test our chain out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSDyVefDaue4"
   },
   "outputs": [],
   "source": [
    "response = rag_graph.invoke({\"question\" : \"When did the Philippines gain independence?\"})\n",
    "response[\"response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### 🎯 Breakout Room - Group Discussion: \n",
    "\n",
    "Why did the model answer the question even when its not related to writings of Dr. Jose Rizal?\n",
    "\n",
    "How can you improve the prompt to respond only within the context?  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### Observations :\n",
    "\n",
    "* In a simple RAG application using LangGraph, the model may answer questions outside the intended scope (e.g., unrelated to Dr. Jose Rizal) because LLMs default to being helpful and will generate answers even when context is missing or weak. To prevent this, improve the prompt by explicitly instructing the model to only answer based on the provided context and to respond with “I cannot answer this question based on the provided documents” when information is not found. This helps ground responses strictly to the retrieved documents and avoids hallucinations.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJtSdDsXZXam"
   },
   "source": [
    "## Task 3: Setting Up LangSmith (Extra! Extra!)\n",
    "\n",
    "Now that we have a chain - we're ready to get started with LangSmith!\n",
    "\n",
    "Create a Langsmith account here(https://smith.langchain.com/) and Setup your API key.\n",
    "\n",
    "We're going to go ahead and use the following `env` variables to get our notebook set up to start reporting.\n",
    "\n",
    "If all you needed was simple monitoring - this is all you would need to do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iqPdBXSBD4a-"
   },
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "PROJECT_NAME = f\"PSI AI Eng - DAY_3 - {uuid4().hex[0:8]}\"\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = PROJECT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ms4msyKLaIr6"
   },
   "source": [
    "### LangSmith API\n",
    "\n",
    "In order to use LangSmith - you will need an API key. You can sign up for a free account on [LangSmith's homepage!](https://www.langchain.com/langsmith)\n",
    "\n",
    "Once you have created your account, Take the navigation option for `Settings` then `API Keys` to create an API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MVq1EYngEMhV",
    "outputId": "5560a787-8114-4c72-b95a-6a776043427a"
   },
   "outputs": [],
   "source": [
    "from langchain.callbacks import LangChainTracer\n",
    "\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass('Enter your LangSmith API key: ')\n",
    "\n",
    "tracer = LangChainTracer()  \n",
    "tracer.project_name = PROJECT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"rag_graph is defined:\", 'rag_graph' in globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qy0MMBLacXv"
   },
   "source": [
    "Let's test our our first generation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3eoqBtBQERXP",
    "outputId": "fc1ea857-6784-4e7e-9fd4-94fbdd9f693a"
   },
   "outputs": [],
   "source": [
    "result = rag_graph.invoke(\n",
    "    {\"question\": \"Who is Capitan Tiago?\"},\n",
    "    config={\"tags\": [\"Demo Run\"], \"callbacks\": [tracer]}\n",
    ")\n",
    "\n",
    "print(\"\\nResponse:\\n\", result['response'])\n",
    "print(\"\\nTracing Project name:\", tracer.project_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZxABFzPr2ny"
   },
   "source": [
    "## Task 4: Examining the Trace in LangSmith!\n",
    "\n",
    "Head on over to your LangSmith web UI to check out how the trace looks in LangSmith!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c52o58AfsLK6"
   },
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "#### 🏗️ Activity #1:\n",
    "\n",
    "Include a screenshot of your trace and explain what it means.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #204B8E; color: white; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "### Observations :\n",
    "![Trace Diagram](images/trace.png)\n",
    "\n",
    "* The LangSmith trace UI lets us:\n",
    "\n",
    "-Visualize each step of the LangChain / LangGraph pipeline\n",
    "-Inspect inputs/outputs to spot formatting or retrieval issues\n",
    "-Debug errors or hallucinations\n",
    "-Optimize prompt, retrieval, and LLM behavior\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
