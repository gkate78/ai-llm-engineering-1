{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Parameter Impact Analysis\n",
    "\n",
    "This notebook demonstrates how different RAG system parameters directly impact evaluation metrics, providing clear visual evidence for why these changes matter.\n",
    "\n",
    "## Key Parameters We'll Test:\n",
    "1. **Chunk Size** - How we split documents into retrievable pieces\n",
    "2. **Embedding Models** - How we convert text to vectors for similarity search\n",
    "3. **Prompt Design** - How we instruct the LLM to generate answers\n",
    "4. **Retrieval Strategy** - How we find relevant context\n",
    "\n",
    "## Metrics We'll Measure:\n",
    "- **Faithfulness** - Does the answer stick to the retrieved context?\n",
    "- **Answer Relevancy** - Is the answer relevant to the question?\n",
    "- **Answer Correctness** - Is the answer factually correct?\n",
    "- **Context Precision** - Are retrieved chunks relevant to the question?\n",
    "- **Context Recall** - Do we retrieve all necessary information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgetpass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m getpass\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from getpass import getpass\n",
    "import openai\n",
    "import nest_asyncio\n",
    "from datasets import Dataset\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.retrievers import MultiQueryRetriever\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness, answer_relevancy, answer_correctness,\n",
    "    context_recall, context_precision\n",
    ")\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Set up OpenAI API\n",
    "openai.api_key = getpass(\"Please provide your OpenAI Key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_csv('data/lotr_testset.csv')\n",
    "test_questions = test_df[\"question\"].values.tolist()\n",
    "test_groundtruths = test_df[\"ground_truth\"].values.tolist()\n",
    "\n",
    "# Load documents\n",
    "loader = PyMuPDFLoader(\"data/lotr.pdf\")\n",
    "all_documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(all_documents)} documents\")\n",
    "print(f\"Test set contains {len(test_questions)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Impact of Chunk Size\n",
    "\n",
    "Chunk size directly affects how much context is available for each retrieval. Too small chunks may miss important context, while too large chunks may include irrelevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_pipeline(chunk_size=1000, chunk_overlap=200, embedding_model=\"text-embedding-ada-002\", prompt_template=None):\n",
    "    \"\"\"Create a RAG pipeline with specified parameters\"\"\"\n",
    "    \n",
    "    # Text splitting\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(all_documents)\n",
    "    \n",
    "    # Embeddings\n",
    "    embeddings = OpenAIEmbeddings(model=embedding_model)\n",
    "    \n",
    "    # Vector store\n",
    "    vector_store = Qdrant.from_documents(\n",
    "        chunks, embeddings, location=\":memory:\", collection_name=f\"LOTR_{chunk_size}\"\n",
    "    )\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 4})\n",
    "    \n",
    "    # LLM\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "    \n",
    "    # Prompt\n",
    "    if prompt_template is None:\n",
    "        prompt_template = \"\"\"Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "    \n",
    "    # Chain\n",
    "    document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "    \n",
    "    return retrieval_chain, len(chunks)\n",
    "\n",
    "def evaluate_pipeline(chain, questions, ground_truths):\n",
    "    \"\"\"Evaluate a RAG pipeline using Ragas metrics\"\"\"\n",
    "    answers = []\n",
    "    contexts = []\n",
    "    \n",
    "    for question in questions:\n",
    "        response = chain.invoke({\"input\": question})\n",
    "        answers.append(response[\"answer\"])\n",
    "        contexts.append([context.page_content for context in response[\"context\"]])\n",
    "    \n",
    "    dataset = Dataset.from_dict({\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truth\": ground_truths\n",
    "    })\n",
    "    \n",
    "    metrics = [faithfulness, answer_relevancy, answer_correctness, context_recall, context_precision]\n",
    "    results = evaluate(dataset, metrics)\n",
    "    \n",
    "    return results.to_pandas()\n",
    "\n",
    "# Test different chunk sizes\n",
    "chunk_sizes = [500, 1000, 2000, 3000]\n",
    "chunk_results = {}\n",
    "chunk_counts = {}\n",
    "\n",
    "print(\"Testing different chunk sizes...\")\n",
    "for chunk_size in chunk_sizes:\n",
    "    print(f\"\\nTesting chunk size: {chunk_size}\")\n",
    "    chain, num_chunks = create_rag_pipeline(chunk_size=chunk_size)\n",
    "    chunk_counts[chunk_size] = num_chunks\n",
    "    results = evaluate_pipeline(chain, test_questions, test_groundtruths)\n",
    "    chunk_results[chunk_size] = results\n",
    "    print(f\"Generated {num_chunks} chunks\")\n",
    "    print(f\"Metrics: {results.iloc[0].to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize chunk size impact\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Impact of Chunk Size on RAG Performance Metrics', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['faithfulness', 'answer_relevancy', 'answer_correctness', 'context_precision']\n",
    "axes = [ax1, ax2, ax3, ax4]\n",
    "\n",
    "for i, (metric, ax) in enumerate(zip(metrics, axes)):\n",
    "    values = [chunk_results[size].iloc[0][metric] for size in chunk_sizes]\n",
    "    \n",
    "    bars = ax.bar(range(len(chunk_sizes)), values, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "    ax.set_xlabel('Chunk Size')\n",
    "    ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "    ax.set_title(f'{metric.replace(\"_\", \" \").title()} vs Chunk Size')\n",
    "    ax.set_xticks(range(len(chunk_sizes)))\n",
    "    ax.set_xticklabels(chunk_sizes)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show chunk counts\n",
    "print(\"\\nChunk counts for different sizes:\")\n",
    "for size, count in chunk_counts.items():\n",
    "    print(f\"Chunk size {size}: {count} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Impact of Embedding Models\n",
    "\n",
    "Different embedding models have different capabilities in understanding semantic relationships and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different embedding models\n",
    "embedding_models = [\"text-embedding-ada-002\", \"text-embedding-3-small\", \"text-embedding-3-large\"]\n",
    "embedding_results = {}\n",
    "\n",
    "print(\"Testing different embedding models...\")\n",
    "for model in embedding_models:\n",
    "    print(f\"\\nTesting embedding model: {model}\")\n",
    "    try:\n",
    "        chain, _ = create_rag_pipeline(embedding_model=model)\n",
    "        results = evaluate_pipeline(chain, test_questions, test_groundtruths)\n",
    "        embedding_results[model] = results\n",
    "        print(f\"Metrics: {results.iloc[0].to_dict()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {model}: {e}\")\n",
    "        embedding_results[model] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embedding model impact\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Impact of Embedding Models on RAG Performance Metrics', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['faithfulness', 'answer_relevancy', 'answer_correctness', 'context_precision']\n",
    "axes = [ax1, ax2, ax3, ax4]\n",
    "\n",
    "for i, (metric, ax) in enumerate(zip(metrics, axes)):\n",
    "    values = []\n",
    "    labels = []\n",
    "    \n",
    "    for model in embedding_models:\n",
    "        if embedding_results[model] is not None:\n",
    "            values.append(embedding_results[model].iloc[0][metric])\n",
    "            labels.append(model.replace('text-embedding-', '').replace('-', ' ').title())\n",
    "    \n",
    "    if values:\n",
    "        bars = ax.bar(range(len(values)), values, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "        ax.set_xlabel('Embedding Model')\n",
    "        ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "        ax.set_title(f'{metric.replace(\"_\", \" \").title()} vs Embedding Model')\n",
    "        ax.set_xticks(range(len(values)))\n",
    "        ax.set_xticklabels(labels, rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, values):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Impact of Prompt Design\n",
    "\n",
    "Different prompts can significantly affect how the LLM interprets and responds to questions using the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different prompt templates\n",
    "prompt_templates = {\n",
    "    \"Basic\": \"\"\"Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\",\n",
    "    \n",
    "    \"Detailed\": \"\"\"You are a helpful assistant. Answer the question based only on the following context. Be thorough and provide specific details when available. If you cannot answer the question with the context, please respond with 'I don't know':\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\",\n",
    "    \n",
    "    \"Concise\": \"\"\"Based on the context below, provide a brief and accurate answer to the question. If the context doesn't contain the answer, say 'I don't know':\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\",\n",
    "    \n",
    "    \"Structured\": \"\"\"Analyze the following context and answer the question step by step:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Steps:\n",
    "1. Identify key information in the context\n",
    "2. Determine if the context contains the answer\n",
    "3. Provide a clear, factual response\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# Test different prompts\n",
    "prompt_results = {}\n",
    "\n",
    "print(\"Testing different prompt designs...\")\n",
    "for prompt_name, prompt_template in prompt_templates.items():\n",
    "    print(f\"\\nTesting prompt: {prompt_name}\")\n",
    "    chain, _ = create_rag_pipeline(prompt_template=prompt_template)\n",
    "    results = evaluate_pipeline(chain, test_questions, test_groundtruths)\n",
    "    prompt_results[prompt_name] = results\n",
    "    print(f\"Metrics: {results.iloc[0].to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prompt design impact\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Impact of Prompt Design on RAG Performance Metrics', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['faithfulness', 'answer_relevancy', 'answer_correctness', 'context_precision']\n",
    "axes = [ax1, ax2, ax3, ax4]\n",
    "prompt_names = list(prompt_results.keys())\n",
    "\n",
    "for i, (metric, ax) in enumerate(zip(metrics, axes)):\n",
    "    values = [prompt_results[name].iloc[0][metric] for name in prompt_names]\n",
    "    \n",
    "    bars = ax.bar(range(len(prompt_names)), values, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "    ax.set_xlabel('Prompt Design')\n",
    "    ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "    ax.set_title(f'{metric.replace(\"_\", \" \").title()} vs Prompt Design')\n",
    "    ax.set_xticks(range(len(prompt_names)))\n",
    "    ax.set_xticklabels(prompt_names, rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Impact of Retrieval Strategy\n",
    "\n",
    "Different retrieval strategies can significantly impact the quality and relevance of retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_retrieval_pipeline():\n",
    "    \"\"\"Create a RAG pipeline with MultiQueryRetriever\"\"\"\n",
    "    \n",
    "    # Basic setup\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_documents(all_documents)\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    vector_store = Qdrant.from_documents(\n",
    "        chunks, embeddings, location=\":memory:\", collection_name=\"LOTR_advanced\"\n",
    "    )\n",
    "    \n",
    "    # Basic retriever\n",
    "    basic_retriever = vector_store.as_retriever(search_kwargs={\"k\": 4})\n",
    "    \n",
    "    # Advanced retriever with MultiQuery\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "    advanced_retriever = MultiQueryRetriever.from_llm(retriever=basic_retriever, llm=llm)\n",
    "    \n",
    "    # Prompt and chain\n",
    "    prompt_template = \"\"\"Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "    document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    \n",
    "    # Create both chains\n",
    "    basic_chain = create_retrieval_chain(basic_retriever, document_chain)\n",
    "    advanced_chain = create_retrieval_chain(advanced_retriever, document_chain)\n",
    "    \n",
    "    return basic_chain, advanced_chain\n",
    "\n",
    "# Test retrieval strategies\n",
    "print(\"Testing different retrieval strategies...\")\n",
    "basic_chain, advanced_chain = create_advanced_retrieval_pipeline()\n",
    "\n",
    "print(\"\\nTesting basic retriever...\")\n",
    "basic_results = evaluate_pipeline(basic_chain, test_questions, test_groundtruths)\n",
    "print(f\"Basic retriever metrics: {basic_results.iloc[0].to_dict()}\")\n",
    "\n",
    "print(\"\\nTesting advanced retriever (MultiQuery)...\")\n",
    "advanced_results = evaluate_pipeline(advanced_chain, test_questions, test_groundtruths)\n",
    "print(f\"Advanced retriever metrics: {advanced_results.iloc[0].to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize retrieval strategy impact\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Impact of Retrieval Strategy on RAG Performance Metrics', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['faithfulness', 'answer_relevancy', 'answer_correctness', 'context_precision']\n",
    "axes = [ax1, ax2, ax3, ax4]\n",
    "strategies = ['Basic Retriever', 'MultiQuery Retriever']\n",
    "\n",
    "for i, (metric, ax) in enumerate(zip(metrics, axes)):\n",
    "    basic_value = basic_results.iloc[0][metric]\n",
    "    advanced_value = advanced_results.iloc[0][metric]\n",
    "    values = [basic_value, advanced_value]\n",
    "    \n",
    "    bars = ax.bar(range(len(strategies)), values, color=['#FF6B6B', '#4ECDC4'])\n",
    "    ax.set_xlabel('Retrieval Strategy')\n",
    "    ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "    ax.set_title(f'{metric.replace(\"_\", \" \").title()} vs Retrieval Strategy')\n",
    "    ax.set_xticks(range(len(strategies)))\n",
    "    ax.set_xticklabels(strategies, rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Comparison Dashboard\n",
    "\n",
    "Let's create a comprehensive view showing how all parameters affect the key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle('Comprehensive RAG Parameter Impact Analysis', fontsize=18, fontweight='bold')\n",
    "\n",
    "# 1. Chunk Size Impact (Faithfulness)\n",
    "ax1 = axes[0, 0]\n",
    "values = [chunk_results[size].iloc[0]['faithfulness'] for size in chunk_sizes]\n",
    "ax1.plot(chunk_sizes, values, 'o-', linewidth=3, markersize=8, color='#FF6B6B')\n",
    "ax1.set_xlabel('Chunk Size (characters)')\n",
    "ax1.set_ylabel('Faithfulness')\n",
    "ax1.set_title('Chunk Size ‚Üí Faithfulness')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Embedding Model Impact (Answer Relevancy)\n",
    "ax2 = axes[0, 1]\n",
    "embedding_values = []\n",
    "embedding_labels = []\n",
    "for model in embedding_models:\n",
    "    if embedding_results[model] is not None:\n",
    "        embedding_values.append(embedding_results[model].iloc[0]['answer_relevancy'])\n",
    "        embedding_labels.append(model.replace('text-embedding-', '').replace('-', ' ').title())\n",
    "ax2.bar(range(len(embedding_values)), embedding_values, color=['#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "ax2.set_xlabel('Embedding Model')\n",
    "ax2.set_ylabel('Answer Relevancy')\n",
    "ax2.set_title('Embedding Model ‚Üí Answer Relevancy')\n",
    "ax2.set_xticks(range(len(embedding_values)))\n",
    "ax2.set_xticklabels(embedding_labels, rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Prompt Design Impact (Answer Correctness)\n",
    "ax3 = axes[0, 2]\n",
    "prompt_values = [prompt_results[name].iloc[0]['answer_correctness'] for name in prompt_names]\n",
    "ax3.bar(range(len(prompt_values)), prompt_values, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "ax3.set_xlabel('Prompt Design')\n",
    "ax3.set_ylabel('Answer Correctness')\n",
    "ax3.set_title('Prompt Design ‚Üí Answer Correctness')\n",
    "ax3.set_xticks(range(len(prompt_values)))\n",
    "ax3.set_xticklabels(prompt_names, rotation=45)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Retrieval Strategy Impact (Context Precision)\n",
    "ax4 = axes[1, 0]\n",
    "retrieval_values = [basic_results.iloc[0]['context_precision'], advanced_results.iloc[0]['context_precision']]\n",
    "ax4.bar(['Basic', 'MultiQuery'], retrieval_values, color=['#FF6B6B', '#4ECDC4'])\n",
    "ax4.set_xlabel('Retrieval Strategy')\n",
    "ax4.set_ylabel('Context Precision')\n",
    "ax4.set_title('Retrieval Strategy ‚Üí Context Precision')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Overall Performance Heatmap\n",
    "ax5 = axes[1, 1]\n",
    "metrics_for_heatmap = ['faithfulness', 'answer_relevancy', 'answer_correctness', 'context_precision']\n",
    "parameters = ['Chunk Size (1000)', 'Embedding (ada-002)', 'Prompt (Basic)', 'Retrieval (Basic)']\n",
    "\n",
    "# Get baseline values\n",
    "baseline_values = [\n",
    "    chunk_results[1000].iloc[0]['faithfulness'],\n",
    "    embedding_results['text-embedding-ada-002'].iloc[0]['answer_relevancy'],\n",
    "    prompt_results['Basic'].iloc[0]['answer_correctness'],\n",
    "    basic_results.iloc[0]['context_precision']\n",
    "]\n",
    "\n",
    "heatmap_data = np.array([baseline_values]).T\n",
    "im = ax5.imshow(heatmap_data, cmap='RdYlGn', aspect='auto')\n",
    "ax5.set_xticks([0])\n",
    "ax5.set_xticklabels(['Baseline'])\n",
    "ax5.set_yticks(range(len(parameters)))\n",
    "ax5.set_yticklabels(parameters)\n",
    "ax5.set_title('Baseline Performance Overview')\n",
    "\n",
    "# Add text annotations\n",
    "for i, value in enumerate(baseline_values):\n",
    "    ax5.text(0, i, f'{value:.3f}', ha='center', va='center', fontweight='bold')\n",
    "\n",
    "# 6. Key Insights\n",
    "ax6 = axes[1, 2]\n",
    "ax6.axis('off')\n",
    "insights_text = '''\n",
    "Key Insights:\n",
    "\n",
    "üîç Chunk Size:\n",
    "‚Ä¢ Too small: Misses context\n",
    "‚Ä¢ Too large: Includes noise\n",
    "‚Ä¢ Optimal: 1000-2000 chars\n",
    "\n",
    "üß† Embedding Models:\n",
    "‚Ä¢ Better models = better semantic understanding\n",
    "‚Ä¢ Direct impact on retrieval quality\n",
    "\n",
    "üìù Prompt Design:\n",
    "‚Ä¢ Clear instructions improve faithfulness\n",
    "‚Ä¢ Structure affects answer quality\n",
    "\n",
    "üîé Retrieval Strategy:\n",
    "‚Ä¢ MultiQuery improves context relevance\n",
    "‚Ä¢ Better retrieval = better answers\n",
    "'''\n",
    "ax6.text(0.05, 0.95, insights_text, transform=ax6.transAxes, fontsize=10,\n",
    "         verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Why These Changes Matter\n",
    "\n",
    "### 1. **Chunk Size Impact**\n",
    "- **Too small chunks** (500 chars): May break up important context, leading to incomplete information retrieval\n",
    "- **Optimal chunks** (1000-2000 chars): Provide sufficient context while maintaining relevance\n",
    "- **Too large chunks** (3000+ chars): Include irrelevant information, reducing precision\n",
    "\n",
    "### 2. **Embedding Model Impact**\n",
    "- **Better embedding models** understand semantic relationships more accurately\n",
    "- **Direct correlation** between embedding quality and retrieval precision\n",
    "- **Newer models** (text-embedding-3) show improved performance over older ones\n",
    "\n",
    "### 3. **Prompt Design Impact**\n",
    "- **Clear instructions** improve faithfulness by guiding the LLM to stick to context\n",
    "- **Structured prompts** can improve answer correctness through step-by-step reasoning\n",
    "- **Prompt engineering** is crucial for controlling LLM behavior\n",
    "\n",
    "### 4. **Retrieval Strategy Impact**\n",
    "- **MultiQueryRetriever** generates multiple query variations, improving context recall\n",
    "- **Better retrieval** directly translates to better answer quality\n",
    "- **Retrieval quality** is the foundation of RAG performance\n",
    "\n",
    "### **The Chain Reaction**\n",
    "1. **Better chunking** ‚Üí More relevant context pieces\n",
    "2. **Better embeddings** ‚Üí More accurate similarity matching\n",
    "3. **Better retrieval** ‚Üí More relevant context for the LLM\n",
    "4. **Better prompts** ‚Üí More faithful and accurate answers\n",
    "5. **Result**: Higher faithfulness, relevancy, and correctness scores\n",
    "\n",
    "This demonstrates why **systematic evaluation** and **parameter optimization** are crucial for building effective RAG systems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
